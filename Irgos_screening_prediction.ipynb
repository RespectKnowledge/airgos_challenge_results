{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Irgos_screening_prediction.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "04efa48131ad4310902ef8d858d8d268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7aa899ddf64f4c48915fefdec39dbb7f",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d4412cf450a04fb9bd9c9f2d26d4cbbf",
              "IPY_MODEL_4b794baf447e47e184f6867026d63daf",
              "IPY_MODEL_e093f712754843fc95762bc3afdc9cdf"
            ]
          }
        },
        "7aa899ddf64f4c48915fefdec39dbb7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d4412cf450a04fb9bd9c9f2d26d4cbbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a6a7f2c419484b22abd0135530ed7784",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b977cd9343a41b28b072d8cef83598f"
          }
        },
        "4b794baf447e47e184f6867026d63daf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d08ae9111a3a496bbe866ceb5294a214",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 354476359,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 354476359,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3a3efcc470824401abe668bf8072031d"
          }
        },
        "e093f712754843fc95762bc3afdc9cdf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c41ee24701a4426ea9303e153d85819d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 338M/338M [00:17&lt;00:00, 22.5MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7577d087a19f41ed89733b2fcea2ba13"
          }
        },
        "a6a7f2c419484b22abd0135530ed7784": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b977cd9343a41b28b072d8cef83598f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d08ae9111a3a496bbe866ceb5294a214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3a3efcc470824401abe668bf8072031d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c41ee24701a4426ea9303e153d85819d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7577d087a19f41ed89733b2fcea2ba13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vvYVo2m7EZ8M",
        "outputId": "c458d4c0-e77e-452b-da2e-7f2ffe8f61ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpeqByImEm4U",
        "outputId": "ecc40083-a820-48bd-f850-982dee060b35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Irgos_challenege2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HHGi3z2zE4c7",
        "outputId": "02f8090f-2461-4600-bb13-70ec7eefd6c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Irgos_challenege2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmHUJIrhE7SC",
        "outputId": "8a354320-9507-4f1a-c41c-ae10ecad9330"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master.zip\n",
            "   creating: airogs-example-algorithm-master/\n",
            " extracting: airogs-example-algorithm-master/.gitattributes  \n",
            "  inflating: airogs-example-algorithm-master/.gitignore  \n",
            "  inflating: airogs-example-algorithm-master/2dmodelairgos.pth  \n",
            " extracting: airogs-example-algorithm-master/build.bat  \n",
            "  inflating: airogs-example-algorithm-master/build.sh  \n",
            "  inflating: airogs-example-algorithm-master/Dockerfile  \n",
            "  inflating: airogs-example-algorithm-master/export.bat  \n",
            "  inflating: airogs-example-algorithm-master/export.sh  \n",
            "  inflating: airogs-example-algorithm-master/irgos_2d2.py  \n",
            "   creating: airogs-example-algorithm-master/output/\n",
            "   creating: airogs-example-algorithm-master/output1/\n",
            "  inflating: airogs-example-algorithm-master/output/multiple-referable-glaucoma-binary.json  \n",
            "  inflating: airogs-example-algorithm-master/output/multiple-referable-glaucoma-likelihoods.json  \n",
            "  inflating: airogs-example-algorithm-master/output/multiple-ungradability-binary.json  \n",
            "  inflating: airogs-example-algorithm-master/output/multiple-ungradability-scores.json  \n",
            "  inflating: airogs-example-algorithm-master/process.py  \n",
            "  inflating: airogs-example-algorithm-master/README.md  \n",
            "  inflating: airogs-example-algorithm-master/requirements.txt  \n",
            "   creating: airogs-example-algorithm-master/test/\n",
            "  inflating: airogs-example-algorithm-master/test.bat  \n",
            "  inflating: airogs-example-algorithm-master/test.sh  \n",
            "  inflating: airogs-example-algorithm-master/test/2dmodelairgos.pth  \n",
            "  inflating: airogs-example-algorithm-master/test/expected-multiple-referable-glaucoma-binary.json  \n",
            "  inflating: airogs-example-algorithm-master/test/expected-multiple-referable-glaucoma-likelihoods.json  \n",
            "  inflating: airogs-example-algorithm-master/test/expected-multiple-ungradability-binary.json  \n",
            "  inflating: airogs-example-algorithm-master/test/expected-multiple-ungradability-scores.json  \n",
            "   creating: airogs-example-algorithm-master/test/images/\n",
            "   creating: airogs-example-algorithm-master/test/images/color-fundus/\n",
            "  inflating: airogs-example-algorithm-master/test/images/color-fundus/phase_1.tiff  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install evalutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2YE0u5QGFDf6",
        "outputId": "f9240a54-b204-4ada-f1f2-f81d0ed1e598"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 23.4 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 7.7 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 3.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 4.3 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Collecting evalutils\n",
            "  Downloading evalutils-0.3.1-py3-none-any.whl (558 kB)\n",
            "\u001b[K     |████████████████████████████████| 558 kB 4.3 MB/s \n",
            "\u001b[?25hCollecting cookiecutter\n",
            "  Downloading cookiecutter-1.7.3-py2.py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.4.1)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from evalutils) (2.4.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from evalutils) (7.1.2)\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 18 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.21.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.0.2)\n",
            "Requirement already satisfied: pandas!=0.24.0 in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.3.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=0.24.0->evalutils) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=0.24.0->evalutils) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=0.24.0->evalutils) (1.15.0)\n",
            "Collecting binaryornot>=0.4.4\n",
            "  Downloading binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting poyo>=0.5.0\n",
            "  Downloading poyo-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting jinja2-time>=0.2.0\n",
            "  Downloading jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: python-slugify>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (5.0.2)\n",
            "Requirement already satisfied: Jinja2<4.0.0,>=2.7 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (2.23.0)\n",
            "Requirement already satisfied: chardet>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from binaryornot>=0.4.4->cookiecutter->evalutils) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<4.0.0,>=2.7->cookiecutter->evalutils) (2.0.1)\n",
            "Collecting arrow\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify>=4.0.0->cookiecutter->evalutils) (1.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from arrow->jinja2-time>=0.2.0->cookiecutter->evalutils) (3.10.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->evalutils) (7.1.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evalutils) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evalutils) (1.1.0)\n",
            "Installing collected packages: arrow, poyo, jinja2-time, binaryornot, SimpleITK, cookiecutter, evalutils\n",
            "Successfully installed SimpleITK-2.1.1 arrow-1.2.2 binaryornot-0.4.4 cookiecutter-1.7.3 evalutils-0.3.1 jinja2-time-0.2.0 poyo-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs==2021.11.20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5TsfAbZ1GSVU",
        "outputId": "b57ca789-f6e9-4347-d06c-c14eabaa80f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagecodecs==2021.11.20\n",
            "  Downloading imagecodecs-2021.11.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.0 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from imagecodecs==2021.11.20) (1.21.5)\n",
            "Installing collected packages: imagecodecs\n",
            "Successfully installed imagecodecs-2021.11.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tifffile==2021.8.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "sZshXlCfGX27",
        "outputId": "8449f9a5-a17c-41ac-85ef-f3febc984890"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tifffile==2021.8.8\n",
            "  Downloading tifffile-2021.8.8-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 5.1 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51 kB 3.2 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61 kB 3.8 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 71 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 92 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 112 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 122 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 133 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 143 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 153 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 163 kB 4.0 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 171 kB 4.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tifffile==2021.8.8) (1.21.5)\n",
            "Installing collected packages: tifffile\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2021.11.2\n",
            "    Uninstalling tifffile-2021.11.2:\n",
            "      Successfully uninstalled tifffile-2021.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed tifffile-2021.8.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tifffile"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import Bool\n",
        "from typing import Dict\n",
        "\n",
        "import SimpleITK\n",
        "import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import tifffile\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/2dmodelairgos.pth'\n",
        "\n",
        "########### define model object for prediction ########\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    r\"\"\" ConvNeXt\n",
        "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
        "          https://arxiv.org/pdf/2201.03545.pdf\n",
        "    Args:\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans=3, num_classes=1000, \n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        out=x\n",
        "        #print(x1.shape)\n",
        "        x = self.head(x)\n",
        "        return x,out\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
        "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
        "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
        "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
        "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
        "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
        "}\n",
        "\n",
        "@register_model\n",
        "def convnext_tiny(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_tiny_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_small(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_small_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
        "    if pretrained:\n",
        "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
        "        url = model_urls['convnext_xlarge_22k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "import sys \n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    img_size = 256\n",
        "    n_frames = 10\n",
        "    \n",
        "    cnn_features = 256\n",
        "    lstm_hidden = 32\n",
        "    \n",
        "    n_fold = 5\n",
        "    n_epochs = 15\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.map = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1)\n",
        "        self.net =convnext_base(pretrained=False)\n",
        "        #checkpoint = torch.load(\"../input/efficientnet-pytorch/efficientnet-b0-08094119.pth\")\n",
        "        #self.net.load_state_dict(checkpoint)\n",
        "        #out = out.view(-1, self.in_planes)\n",
        "        self.net.head = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #x = F.relu(self.map(x))\n",
        "        out,out1 = self.net(x)\n",
        "        return out,out1\n",
        "    \n",
        "model=CNN()\n",
        "\n",
        "\n",
        "\n",
        "################### variational autoencoder model\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "#from example_algos.models.nets import BasicEncoder, BasicGenerator\n",
        "\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NoOp(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"NoOp Pytorch Module.\n",
        "        Forwards the given input as is.\n",
        "        \"\"\"\n",
        "        super(NoOp, self).__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv Pytorch Conv Module\n",
        "        Has can have a Conv Op, a Normlization Op and a Non Linearity:\n",
        "        x = conv(x)\n",
        "        x = some_norm(x)\n",
        "        x = nonlin(x)\n",
        "\n",
        "        Args:\n",
        "            in_channels ([int]): [Number on input channels/ feature maps]\n",
        "            out_channels ([int]): [Number of ouput channels/ feature maps]\n",
        "            conv_op ([torch.nn.Module], optional): [Conv operation]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...)]. Defaults to None.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...)]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "        self.activation_params = activation_params\n",
        "        if self.activation_params is None:\n",
        "            self.activation_params = {}\n",
        "        self.normalization_params = normalization_params\n",
        "        if self.normalization_params is None:\n",
        "            self.normalization_params = {}\n",
        "\n",
        "        self.conv = None\n",
        "        if conv_op is not None and not isinstance(conv_op, str):\n",
        "            self.conv = conv_op(in_channels, out_channels, **self.conv_params)\n",
        "\n",
        "        self.normalization = None\n",
        "        if normalization_op is not None and not isinstance(normalization_op, str):\n",
        "            self.normalization = normalization_op(out_channels, **self.normalization_params)\n",
        "\n",
        "        self.activation = None\n",
        "        if activation_op is not None and not isinstance(activation_op, str):\n",
        "            self.activation = activation_op(**self.activation_params)\n",
        "\n",
        "    def forward(self, input, conv_add_input=None, normalization_add_input=None, activation_add_input=None):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        if self.conv is not None:\n",
        "            if conv_add_input is None:\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.conv(x, **conv_add_input)\n",
        "\n",
        "        if self.normalization is not None:\n",
        "            if normalization_add_input is None:\n",
        "                x = self.normalization(x)\n",
        "            else:\n",
        "                x = self.normalization(x, **normalization_add_input)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            if activation_add_input is None:\n",
        "                x = self.activation(x)\n",
        "            else:\n",
        "                x = self.activation(x, **activation_add_input)\n",
        "\n",
        "        # nn.functional.dropout(x, p=0.95, training=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs: int,\n",
        "        n_featmaps: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size)\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions]\n",
        "            n_featmaps ([type]): [Feature map size of the conv]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.n_convs):\n",
        "            conv_layer = ConvModule(\n",
        "                n_featmaps,\n",
        "                n_featmaps,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=conv_params,\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "            self.conv_list.append(conv_layer)\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        for conv_layer in self.conv_list:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs,\n",
        "        n_featmaps,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size) and a skip/ residual connection:\n",
        "        x = input\n",
        "        x = conv_block(x)\n",
        "        out = x + input\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions in the conv block]\n",
        "            n_featmaps ([type]): [Feature map size of the conv block]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_block = ConvBlock(\n",
        "            n_featmaps,\n",
        "            n_convs,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        out = x + input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Basic Generator\n",
        "class BasicGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(256, 128, 64),\n",
        "        upsample_op=nn.ConvTranspose2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Generator/ Decoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used.\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (256, 128, 64).\n",
        "            upsample_op ([torch.nn.Module], optional): [Upsampling operation used, to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If Latent dimesion is a z_dim x 1 x 1 vector (True) or if allows spatial resolution not to be 1x1 (z_dim x H x W) (False) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        super(BasicGenerator, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_ = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        elif len(fmap_sizes) < 2:\n",
        "            raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        # We need to know how many layers we will use at the beginning\n",
        "        input_size_new = input_size_ // (2 ** len(fmap_sizes))\n",
        "        if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "            raise AttributeError(\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### Start block\n",
        "        start_block = []\n",
        "\n",
        "        if not to_1x1:\n",
        "            kernel_size_start = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_start = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.start = ConvModule(\n",
        "                z_dim,\n",
        "                h_size_bot,\n",
        "                conv_op=upsample_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_start, stride=1, padding=0, bias=False),\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "\n",
        "            input_size_new = input_size_new * 2\n",
        "        else:\n",
        "            self.start = NoOp()\n",
        "\n",
        "        ### Middle block (Done until we reach ? x input_size/2 x input_size/2)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=upsample_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new * 2\n",
        "\n",
        "        ### End block\n",
        "        self.end = ConvModule(\n",
        "            h_size_bot,\n",
        "            n_channels,\n",
        "            conv_op=upsample_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=None,\n",
        "            activation_op=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Basic Encoder\n",
        "class BasicEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(64, 128, 256),\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Encoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used. \n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (64, 128, 256).\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(BasicEncoder, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_new = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        # elif len(fmap_sizes) < 2:\n",
        "        #     raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        ### Start block\n",
        "        self.start = ConvModule(\n",
        "            n_channels,\n",
        "            h_size_bot,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params={},\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "        input_size_new = input_size_new // 2\n",
        "\n",
        "        ### Middle block (Done until we reach ? x 4 x 4)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=conv_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new // 2\n",
        "\n",
        "            if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "                raise (\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### End block\n",
        "        if not to_1x1:\n",
        "            kernel_size_end = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_end = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.end = ConvModule(\n",
        "                h_size_bot,\n",
        "                z_dim,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_end, stride=1, padding=0, bias=False),\n",
        "                normalization_op=None,\n",
        "                activation_op=None,\n",
        "            )\n",
        "\n",
        "            if to_1x1:\n",
        "                self.output_size = (z_dim, 1, 1)\n",
        "            else:\n",
        "                self.output_size = (z_dim, *[i - (j - 1) for i, j in zip(input_size_new, kernel_size_end)])\n",
        "        else:\n",
        "            self.end = NoOp()\n",
        "            self.output_size = input_size_new\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic VAE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim * 2,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, sample=True, no_dist=False, **kwargs):\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        mu, log_std = torch.chunk(y1, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        z_dist = dist.Normal(mu, std)\n",
        "        if sample:\n",
        "            z_sample = z_dist.rsample()\n",
        "        else:\n",
        "            z_sample = mu\n",
        "\n",
        "        x_rec = self.dec(z_sample)\n",
        "\n",
        "        if no_dist:\n",
        "            return x_rec\n",
        "        else:\n",
        "            return x_rec, z_dist\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a sample and returns the paramters for the approx inference dist. (Normal)\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): The input to encode\n",
        "\n",
        "        Returns:\n",
        "            mu : The mean used to parameterized a Normal distribution\n",
        "            std: The standard deviation used to parameterized a Normal distribution\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        mu, log_std = torch.chunk(enc, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample, used the generative model (decode = mu_{gen}(z) as used in p(x|z) = N(x | mu_{gen}(z), 1) ).\n",
        "\n",
        "        Args:\n",
        "            inpt ([type]): A sample from the latent space to decode\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        x_rec = self.dec(inpt, **kwargs)\n",
        "        return x_rec\n",
        "\n",
        "\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=1024,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic AE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        x_rec = self.dec(y1)\n",
        "\n",
        "        return x_rec\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a input sample to a latent space sample\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): Input sample\n",
        "\n",
        "        Returns:\n",
        "            enc: Encoded input sample in the latent space\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample back to the input space\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): [Latent space sample]\n",
        "\n",
        "        Returns:\n",
        "            [rec]: [Encoded latent sample back in the input space]\n",
        "        \"\"\"\n",
        "        rec = self.dec(inpt, **kwargs)\n",
        "        return rec\n",
        "\n",
        "z_dim=512\n",
        "model_feature_map_sizes=(16, 64, 256, 1024)\n",
        "    \n",
        "import torch\n",
        "input_shape=((1, 1, 128, 128))\n",
        "#input_shape(0).shape\n",
        "#c,h,y=input_shape.size()[0],input_shape.size()[1],input_shape.size()[2]\n",
        "\n",
        "model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)\n",
        "patha='/content/drive/MyDrive/Irgos_challenege2022/model_weights.pth'\n",
        "trained=torch.load(patha)\n",
        "model1.load_state_dict(trained)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model=nn.DataParallel(model)\n",
        "#model=model.to(device)\n",
        "from evalutils import ClassificationAlgorithm\n",
        "from evalutils.validators import (\n",
        "    UniquePathIndicesValidator,\n",
        "    UniqueImagesValidator,\n",
        ")\n",
        "from evalutils.io import ImageLoader\n",
        "\n",
        "\n",
        "class DummyLoader(ImageLoader):\n",
        "    @staticmethod\n",
        "    def load_image(fname):\n",
        "        return str(fname)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def hash_image(image):\n",
        "        return hash(image)\n",
        "\n",
        "\n",
        "class airogs_algorithm(ClassificationAlgorithm):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            validators=dict(\n",
        "                input_image=(\n",
        "                    UniqueImagesValidator(),\n",
        "                    UniquePathIndicesValidator(),\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self._file_loaders = dict(input_image=DummyLoader())\n",
        "\n",
        "        self.output_keys = [\"multiple-referable-glaucoma-likelihoods\", \n",
        "                            \"multiple-referable-glaucoma-binary\",\n",
        "                            \"multiple-ungradability-scores\",\n",
        "                            \"multiple-ungradability-binary\"]\n",
        "    \n",
        "    def load(self):\n",
        "        for key, file_loader in self._file_loaders.items():\n",
        "            fltr = (\n",
        "                self._file_filters[key] if key in self._file_filters else None\n",
        "            )\n",
        "            self._cases[key] = self._load_cases(\n",
        "                folder=Path(\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/\"),\n",
        "                file_loader=file_loader,\n",
        "                file_filter=fltr,\n",
        "            )\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def combine_dicts(self, dicts):\n",
        "        out = {}\n",
        "        for d in dicts:\n",
        "            for k, v in d.items():\n",
        "                if k not in out:\n",
        "                    out[k] = []\n",
        "                out[k].append(v)\n",
        "        return out\n",
        "    \n",
        "    def process_case(self, *, idx, case):\n",
        "        # Load and test the image(s) for this case\n",
        "        if case.path.suffix == '.tiff':\n",
        "            results = []\n",
        "            #predc=[]\n",
        "            with tifffile.TiffFile(case.path) as stack:\n",
        "                for page in tqdm.tqdm(stack.pages):\n",
        "                    input_image_array = page.asarray()\n",
        "                    \n",
        "                    #print(input_image_array.shape)\n",
        "                    input_image_array= cv2.resize(input_image_array, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "                    input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "                    #np_img=input_image_array\n",
        "                    #torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "                    #torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "                    #print(torch_array.shape)\n",
        "                    #plt.imshow(input_image_array)\n",
        "                    #print(input_image_array.min())\n",
        "                    #print(input_image_array.max())\n",
        "                    #p1,p2=self.predict(input_image_array=input_image_array)\n",
        "                    results.append(self.predict(input_image_array=input_image_array))\n",
        "                    #predc.append(p2)\n",
        "        else:\n",
        "            input_image = SimpleITK.ReadImage(str(case.path))\n",
        "            input_image_array = SimpleITK.GetArrayFromImage(input_image)\n",
        "            print(input_image_array.shape)\n",
        "            results = [self.predict(input_image_array=input_image_array)]\n",
        "        \n",
        "        results = self.combine_dicts(results)\n",
        "\n",
        "        # Test classification output\n",
        "        if not isinstance(results, dict):\n",
        "            raise ValueError(\"Expected a dictionary as output\")\n",
        "\n",
        "        return results\n",
        "    \n",
        "    #pc11=[]\n",
        "\n",
        "    def predict(self, *, input_image_array: np.ndarray) -> Dict:\n",
        "        # From here, use the input_image to predict the output\n",
        "        # We are using a not-so-smart algorithm to predict the output, you'll want to do your model inference here\n",
        "\n",
        "        # Replace starting here\n",
        "        #print(input_image_array)\n",
        "        #np_img=input_image_array\n",
        "        #torch_array=torch.from_numpy(np_img).squeeze(axis=0)\n",
        "        #np_img=input_image_array\n",
        "        #torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "        #torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "        #torch_array=torch_array.float().to(device)\n",
        "        ####### load pytorch model for prediction\n",
        "        #model=CNN()\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        #pretrained=torch.load(path)\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        #model.load_state_dict(pretrained)\n",
        "        #y_pred,_=model(torch_array)\n",
        "        #pc1= torch.softmax(y_pred, dim=1)\n",
        "        #predict1=pc1.detach().cpu().numpy()\n",
        "        #print(predict1)\n",
        "        #pc11.append(predict1)\n",
        "        \n",
        "        #rg_likelihood = ((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min())).mean()\n",
        "        #rg_binary = bool(rg_likelihood > .2)\n",
        "        #ungradability_score = rg_likelihood * 15\n",
        "        #ungradability_binary = bool(rg_likelihood < .2)\n",
        "        \n",
        "        np_img=input_image_array\n",
        "        torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "        torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "        torch_array=torch_array.float().to(device)\n",
        "        ####### load pytorch model for prediction\n",
        "        model=CNN()\n",
        "        model=nn.DataParallel(model)\n",
        "        model=model.to(device)\n",
        "        pretrained=torch.load(path1)\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        model.load_state_dict(pretrained)\n",
        "        model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)\n",
        "        #patha='/content/drive/MyDrive/Irgos_challenege2022/model_weights.pth'\n",
        "        trained=torch.load(patha)\n",
        "        model1.load_state_dict(trained)\n",
        "        with torch.no_grad():\n",
        "          y_pred,_=model(torch_array)\n",
        "          pc1= torch.softmax(y_pred, dim=1)\n",
        "          pc1=pc1.squeeze(axis=0)\n",
        "          predic0,predic1=pc1.detach().cpu().numpy()\n",
        "          #p2=predic1\n",
        "          #predict11.append(p2)\n",
        "          #predic00.append(predic0)\n",
        "          input_image_array1= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "          input_image_array1=((input_image_array1 - input_image_array1.min()) / (input_image_array1.max() - input_image_array1.min()))\n",
        "          #print(input_image_array1.shape)\n",
        "          np_img1=input_image_array1\n",
        "          torch_array1=torch.from_numpy(np_img1).unsqueeze(axis=0)\n",
        "          torch_array1=torch_array1.permute(0, 3, 1, 2)\n",
        "          torch_array1=torch_array1.float().to(device)\n",
        "          model1=model1.to(device)\n",
        "          torch_array1=torch_array1.to(device)\n",
        "          batch_rec = model1(torch_array1)\n",
        "          loss = torch.mean(torch.pow(torch_array1 - batch_rec, 2), dim=(1, 2, 3))\n",
        "          #slice_scores += loss.cpu().tolist()\n",
        "          #batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()\n",
        "          ###### output probabilties computed here\n",
        "          rg_likelihood=float(predic1)\n",
        "          rg_binary=(rg_likelihood>0.5)\n",
        "          #rg_likelihood=str(rg_likelihood1)\n",
        "          #rg_binary=str(rg_binary1)\n",
        "          un_gradable=predic0\n",
        "          predb0=(un_gradable>0.5)\n",
        "          ungradability_score=np.array(loss.cpu().numpy()*predb0)\n",
        "          #ungradability_score=str(ungradability_score1)\n",
        "          ungradability_score=float(np.squeeze(ungradability_score))\n",
        "          ungradability_binary=bool(ungradability_score>0)\n",
        "          #ungradability_score=str(ungradability_binary1)\n",
        "\n",
        "          #print(rg_likelihood)\n",
        "          #print(rg_binary)\n",
        "          #print(ungradability_score)\n",
        "          #print(ungradability_binary)\n",
        "\n",
        "\n",
        "          #likelihood1.append(likelihood)\n",
        "          #rg_binary1.append(rg_binary)\n",
        "          #ungradability_score1.append(ungradability_score)\n",
        "          #ungradability_binary1.append(ungradability_binary)\n",
        "        \n",
        "        \n",
        "        # to here with your inference algorithm\n",
        "\n",
        "        out = {\n",
        "            \"multiple-referable-glaucoma-likelihoods\": rg_likelihood,\n",
        "            \"multiple-referable-glaucoma-binary\": rg_binary,\n",
        "            \"multiple-ungradability-scores\": ungradability_score,\n",
        "            \"multiple-ungradability-binary\": ungradability_binary\n",
        "        }\n",
        "\n",
        "        return out\n",
        "\n",
        "    def save(self):\n",
        "        for key in self.output_keys:\n",
        "            with open(f\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/output1/{key}.json\", \"w\") as f:\n",
        "                out = []\n",
        "                for case_result in self._case_results:\n",
        "                    out += case_result[key]\n",
        "                json.dump(out, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    airogs_algorithm().process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wTKz4cd7FDlq",
        "outputId": "76602039-b41a-4828-d81f-295d34be9f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:38<00:00,  3.81s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5oUNhYrE2vbD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "5ef-UnSP2vna"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict\n",
        "\n",
        "import SimpleITK\n",
        "import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import tifffile\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "path1='/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/2dmodelairgos.pth'\n",
        "\n",
        "########### define model object for prediction ########\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    r\"\"\" ConvNeXt\n",
        "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
        "          https://arxiv.org/pdf/2201.03545.pdf\n",
        "    Args:\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans=3, num_classes=1000, \n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        out=x\n",
        "        #print(x1.shape)\n",
        "        x = self.head(x)\n",
        "        return x,out\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
        "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
        "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
        "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
        "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
        "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
        "}\n",
        "\n",
        "@register_model\n",
        "def convnext_tiny(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_tiny_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_small(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_small_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
        "    if pretrained:\n",
        "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
        "        url = model_urls['convnext_xlarge_22k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "import sys \n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    img_size = 256\n",
        "    n_frames = 10\n",
        "    \n",
        "    cnn_features = 256\n",
        "    lstm_hidden = 32\n",
        "    \n",
        "    n_fold = 5\n",
        "    n_epochs = 15\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.map = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1)\n",
        "        self.net =convnext_base(pretrained=True)\n",
        "        #checkpoint = torch.load(\"../input/efficientnet-pytorch/efficientnet-b0-08094119.pth\")\n",
        "        #self.net.load_state_dict(checkpoint)\n",
        "        #out = out.view(-1, self.in_planes)\n",
        "        self.net.head = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #x = F.relu(self.map(x))\n",
        "        out,out1 = self.net(x)\n",
        "        return out,out1\n",
        "    \n",
        "model=CNN()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from evalutils.io import ImageLoader\n",
        "\n",
        "\n",
        "class DummyLoader(ImageLoader):\n",
        "    @staticmethod\n",
        "    def load_image(fname):\n",
        "        return str(fname)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def hash_image(image):\n",
        "        return hash(image)\n",
        "\n",
        "\n",
        "class airogs_algorithm(ClassificationAlgorithm):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            validators=dict(\n",
        "                input_image=(\n",
        "                    UniqueImagesValidator(),\n",
        "                    UniquePathIndicesValidator(),\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self._file_loaders = dict(input_image=DummyLoader())\n",
        "\n",
        "        self.output_keys = [\"multiple-referable-glaucoma-likelihoods\", \n",
        "                            \"multiple-referable-glaucoma-binary\",\n",
        "                            \"multiple-ungradability-scores\",\n",
        "                            \"multiple-ungradability-binary\"]\n",
        "    \n",
        "    def load(self):\n",
        "        for key, file_loader in self._file_loaders.items():\n",
        "            fltr = (\n",
        "                self._file_filters[key] if key in self._file_filters else None\n",
        "            )\n",
        "            self._cases[key] = self._load_cases(\n",
        "                folder=Path(\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/\"),\n",
        "                file_loader=file_loader,\n",
        "                file_filter=fltr,\n",
        "            )\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def combine_dicts(self, dicts):\n",
        "        out = {}\n",
        "        for d in dicts:\n",
        "            for k, v in d.items():\n",
        "                if k not in out:\n",
        "                    out[k] = []\n",
        "                out[k].append(v)\n",
        "        return out\n",
        "    \n",
        "path=\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/phase_1.tiff\"\n",
        "# Load and test the image(s) for this case\n",
        "with tifffile.TiffFile(path) as stack:\n",
        "  predict11=[]\n",
        "  for page in stack.pages:\n",
        "    input_image_array = page.asarray()\n",
        "    #print(input_image_array.shape)\n",
        "    input_image_array= cv2.resize(input_image_array, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "    input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "    print(input_image_array.shape)\n",
        "    np_img=input_image_array\n",
        "    torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "    torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "    torch_array=torch_array.float().to(device)\n",
        "    ####### load pytorch model for prediction\n",
        "    model=CNN()\n",
        "    model=nn.DataParallel(model)\n",
        "    model=model.to(device)\n",
        "    pretrained=torch.load(path1)\n",
        "    #model=nn.DataParallel(model)\n",
        "    #model=model.to(device)\n",
        "    model.load_state_dict(pretrained)\n",
        "    y_pred,_=model(torch_array)\n",
        "    pc1= torch.softmax(y_pred, dim=1)\n",
        "    pc1=pc1.squeeze(axis=0)\n",
        "    predic0,predic1=pc1.detach().cpu().numpy()\n",
        "    p2=predic1\n",
        "    predict11.append(p2)\n",
        "                    \n",
        "    "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260,
          "referenced_widgets": [
            "04efa48131ad4310902ef8d858d8d268",
            "7aa899ddf64f4c48915fefdec39dbb7f",
            "d4412cf450a04fb9bd9c9f2d26d4cbbf",
            "4b794baf447e47e184f6867026d63daf",
            "e093f712754843fc95762bc3afdc9cdf",
            "a6a7f2c419484b22abd0135530ed7784",
            "0b977cd9343a41b28b072d8cef83598f",
            "d08ae9111a3a496bbe866ceb5294a214",
            "3a3efcc470824401abe668bf8072031d",
            "c41ee24701a4426ea9303e153d85819d",
            "7577d087a19f41ed89733b2fcea2ba13"
          ]
        },
        "id": "FvlxDYVgLciP",
        "outputId": "f29e7245-7499-4897-fb5c-4b2e36df512d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\" to /root/.cache/torch/hub/checkpoints/convnext_base_1k_224_ema.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04efa48131ad4310902ef8d858d8d268",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/338M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n",
            "(256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array3=np.array(predict11)"
      ],
      "metadata": {
        "id": "KzvmQnM9QS4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IoJLT_8sQW_a",
        "outputId": "0178faf6-6e46-43ad-a1d2-47c9589601f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4.7530200e-05, 4.4433564e-05, 5.8574675e-05, 9.9777609e-01,\n",
              "       4.4186116e-05, 9.9942172e-01, 4.3568405e-05, 4.1985750e-05,\n",
              "       4.7342921e-05, 4.4428223e-05], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp=(array3>0.5)"
      ],
      "metadata": {
        "id": "AEq6kiJINuQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1lRpt1NNnDl",
        "outputId": "3dd4d538-a8d7-45d6-c889-2d4b2f999d77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False,  True, False,  True, False, False, False,\n",
              "       False])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Irgos_challenege2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd6zk1qvJaXZ",
        "outputId": "f49d5db6-45ec-494b-afbd-a32b09505a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Irgos_challenege2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Irgos_challenege2022/gradable_folder1.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MaBQmMvRUfGo",
        "outputId": "f03534f9-5588-4a23-b457-aa0e832e13ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Irgos_challenege2022/gradable_folder1.zip\n",
            "   creating: gradable_folder1/\n",
            "  inflating: gradable_folder1/TRAIN000000.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000001.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000002.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000034.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000060.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000068.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000100.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000106.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000122.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000127.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000140.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000188.jpg  \n",
            "  inflating: gradable_folder1/TRAIN000195.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/gradable_folder1'\n",
        "lstdir=os.listdir(path)\n",
        "predict111=[]\n",
        "for i in lstdir:\n",
        "  print(i)\n",
        "  pathim=os.path.join(path,i)\n",
        "  input_image_array=cv2.imread(pathim)\n",
        "  input_image_array= cv2.resize(input_image_array, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "  input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "  print(input_image_array.shape)\n",
        "  np_img=input_image_array\n",
        "  torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "  torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "  torch_array=torch_array.float().to(device)\n",
        "  ####### load pytorch model for prediction\n",
        "  model=CNN()\n",
        "  model=nn.DataParallel(model)\n",
        "  model=model.to(device)\n",
        "  pretrained=torch.load(path1)\n",
        "  #model=nn.DataParallel(model)\n",
        "  #model=model.to(device)\n",
        "  model.load_state_dict(pretrained)\n",
        "  y_pred,_=model(torch_array)\n",
        "  pc1= torch.softmax(y_pred, dim=1)\n",
        "  pc1=pc1.squeeze(axis=0)\n",
        "  predic0,predic1=pc1.detach().cpu().numpy()\n",
        "  p2=predic1\n",
        "  predict111.append(p2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9iPdFtDsUjZO",
        "outputId": "3956f8e2-0501-4762-c29c-f4071f27de64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN000000.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000001.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000002.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000034.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000060.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000068.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000100.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000106.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000122.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000127.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000140.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000188.jpg\n",
            "(256, 256, 3)\n",
            "TRAIN000195.jpg\n",
            "(256, 256, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "array5=np.array(predict111)"
      ],
      "metadata": {
        "id": "miBxCHV6VWif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "array5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNWCtsY0Vhyn",
        "outputId": "22bc1a61-0c97-4012-8a17-81aaacdb1e36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5.6939451e-05, 6.6662273e-05, 4.5404362e-05, 9.9973089e-01,\n",
              "       9.9966407e-01, 9.9967337e-01, 9.9976271e-01, 9.9088085e-01,\n",
              "       9.9974817e-01, 9.9645680e-01, 9.9969435e-01, 9.9972612e-01,\n",
              "       9.9973375e-01], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pp=(array5>0.5)\n",
        "pp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1zi8ut9VbwO",
        "outputId": "8245c02f-0f14-4cdc-e13e-ecb44f7dbbac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YwBWRPvYdmH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3TiHjYsedmK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "################### variational autoencoder model\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "#from example_algos.models.nets import BasicEncoder, BasicGenerator\n",
        "\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NoOp(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"NoOp Pytorch Module.\n",
        "        Forwards the given input as is.\n",
        "        \"\"\"\n",
        "        super(NoOp, self).__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv Pytorch Conv Module\n",
        "        Has can have a Conv Op, a Normlization Op and a Non Linearity:\n",
        "        x = conv(x)\n",
        "        x = some_norm(x)\n",
        "        x = nonlin(x)\n",
        "\n",
        "        Args:\n",
        "            in_channels ([int]): [Number on input channels/ feature maps]\n",
        "            out_channels ([int]): [Number of ouput channels/ feature maps]\n",
        "            conv_op ([torch.nn.Module], optional): [Conv operation]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...)]. Defaults to None.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...)]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "        self.activation_params = activation_params\n",
        "        if self.activation_params is None:\n",
        "            self.activation_params = {}\n",
        "        self.normalization_params = normalization_params\n",
        "        if self.normalization_params is None:\n",
        "            self.normalization_params = {}\n",
        "\n",
        "        self.conv = None\n",
        "        if conv_op is not None and not isinstance(conv_op, str):\n",
        "            self.conv = conv_op(in_channels, out_channels, **self.conv_params)\n",
        "\n",
        "        self.normalization = None\n",
        "        if normalization_op is not None and not isinstance(normalization_op, str):\n",
        "            self.normalization = normalization_op(out_channels, **self.normalization_params)\n",
        "\n",
        "        self.activation = None\n",
        "        if activation_op is not None and not isinstance(activation_op, str):\n",
        "            self.activation = activation_op(**self.activation_params)\n",
        "\n",
        "    def forward(self, input, conv_add_input=None, normalization_add_input=None, activation_add_input=None):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        if self.conv is not None:\n",
        "            if conv_add_input is None:\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.conv(x, **conv_add_input)\n",
        "\n",
        "        if self.normalization is not None:\n",
        "            if normalization_add_input is None:\n",
        "                x = self.normalization(x)\n",
        "            else:\n",
        "                x = self.normalization(x, **normalization_add_input)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            if activation_add_input is None:\n",
        "                x = self.activation(x)\n",
        "            else:\n",
        "                x = self.activation(x, **activation_add_input)\n",
        "\n",
        "        # nn.functional.dropout(x, p=0.95, training=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs: int,\n",
        "        n_featmaps: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size)\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions]\n",
        "            n_featmaps ([type]): [Feature map size of the conv]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.n_convs):\n",
        "            conv_layer = ConvModule(\n",
        "                n_featmaps,\n",
        "                n_featmaps,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=conv_params,\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "            self.conv_list.append(conv_layer)\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        for conv_layer in self.conv_list:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs,\n",
        "        n_featmaps,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size) and a skip/ residual connection:\n",
        "        x = input\n",
        "        x = conv_block(x)\n",
        "        out = x + input\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions in the conv block]\n",
        "            n_featmaps ([type]): [Feature map size of the conv block]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_block = ConvBlock(\n",
        "            n_featmaps,\n",
        "            n_convs,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        out = x + input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Basic Generator\n",
        "class BasicGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(256, 128, 64),\n",
        "        upsample_op=nn.ConvTranspose2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Generator/ Decoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used.\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (256, 128, 64).\n",
        "            upsample_op ([torch.nn.Module], optional): [Upsampling operation used, to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If Latent dimesion is a z_dim x 1 x 1 vector (True) or if allows spatial resolution not to be 1x1 (z_dim x H x W) (False) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        super(BasicGenerator, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_ = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        elif len(fmap_sizes) < 2:\n",
        "            raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        # We need to know how many layers we will use at the beginning\n",
        "        input_size_new = input_size_ // (2 ** len(fmap_sizes))\n",
        "        if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "            raise AttributeError(\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### Start block\n",
        "        start_block = []\n",
        "\n",
        "        if not to_1x1:\n",
        "            kernel_size_start = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_start = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.start = ConvModule(\n",
        "                z_dim,\n",
        "                h_size_bot,\n",
        "                conv_op=upsample_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_start, stride=1, padding=0, bias=False),\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "\n",
        "            input_size_new = input_size_new * 2\n",
        "        else:\n",
        "            self.start = NoOp()\n",
        "\n",
        "        ### Middle block (Done until we reach ? x input_size/2 x input_size/2)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=upsample_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new * 2\n",
        "\n",
        "        ### End block\n",
        "        self.end = ConvModule(\n",
        "            h_size_bot,\n",
        "            n_channels,\n",
        "            conv_op=upsample_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=None,\n",
        "            activation_op=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Basic Encoder\n",
        "class BasicEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(64, 128, 256),\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Encoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used. \n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (64, 128, 256).\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(BasicEncoder, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_new = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        # elif len(fmap_sizes) < 2:\n",
        "        #     raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        ### Start block\n",
        "        self.start = ConvModule(\n",
        "            n_channels,\n",
        "            h_size_bot,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params={},\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "        input_size_new = input_size_new // 2\n",
        "\n",
        "        ### Middle block (Done until we reach ? x 4 x 4)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=conv_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new // 2\n",
        "\n",
        "            if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "                raise (\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### End block\n",
        "        if not to_1x1:\n",
        "            kernel_size_end = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_end = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.end = ConvModule(\n",
        "                h_size_bot,\n",
        "                z_dim,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_end, stride=1, padding=0, bias=False),\n",
        "                normalization_op=None,\n",
        "                activation_op=None,\n",
        "            )\n",
        "\n",
        "            if to_1x1:\n",
        "                self.output_size = (z_dim, 1, 1)\n",
        "            else:\n",
        "                self.output_size = (z_dim, *[i - (j - 1) for i, j in zip(input_size_new, kernel_size_end)])\n",
        "        else:\n",
        "            self.end = NoOp()\n",
        "            self.output_size = input_size_new\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic VAE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim * 2,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, sample=True, no_dist=False, **kwargs):\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        mu, log_std = torch.chunk(y1, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        z_dist = dist.Normal(mu, std)\n",
        "        if sample:\n",
        "            z_sample = z_dist.rsample()\n",
        "        else:\n",
        "            z_sample = mu\n",
        "\n",
        "        x_rec = self.dec(z_sample)\n",
        "\n",
        "        if no_dist:\n",
        "            return x_rec\n",
        "        else:\n",
        "            return x_rec, z_dist\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a sample and returns the paramters for the approx inference dist. (Normal)\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): The input to encode\n",
        "\n",
        "        Returns:\n",
        "            mu : The mean used to parameterized a Normal distribution\n",
        "            std: The standard deviation used to parameterized a Normal distribution\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        mu, log_std = torch.chunk(enc, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample, used the generative model (decode = mu_{gen}(z) as used in p(x|z) = N(x | mu_{gen}(z), 1) ).\n",
        "\n",
        "        Args:\n",
        "            inpt ([type]): A sample from the latent space to decode\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        x_rec = self.dec(inpt, **kwargs)\n",
        "        return x_rec\n",
        "\n",
        "\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=1024,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic AE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        x_rec = self.dec(y1)\n",
        "\n",
        "        return x_rec\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a input sample to a latent space sample\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): Input sample\n",
        "\n",
        "        Returns:\n",
        "            enc: Encoded input sample in the latent space\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample back to the input space\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): [Latent space sample]\n",
        "\n",
        "        Returns:\n",
        "            [rec]: [Encoded latent sample back in the input space]\n",
        "        \"\"\"\n",
        "        rec = self.dec(inpt, **kwargs)\n",
        "        return rec\n",
        "\n",
        "z_dim=512\n",
        "model_feature_map_sizes=(16, 64, 256, 1024)\n",
        "    \n",
        "import torch\n",
        "input_shape=((1, 1, 128, 128))\n",
        "#input_shape(0).shape\n",
        "#c,h,y=input_shape.size()[0],input_shape.size()[1],input_shape.size()[2]\n",
        "\n",
        "model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)"
      ],
      "metadata": {
        "id": "A8nUMCJfdmOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patha='/content/drive/MyDrive/Irgos_challenege2022/model_weights.pth'\n",
        "trained=torch.load(patha)\n",
        "model1.load_state_dict(trained)"
      ],
      "metadata": {
        "id": "0iuvl_IKe2tP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#patha='/content/drive/MyDrive/Irgos_challenege2022/model_weights.pth'\n",
        "#trained=torch.load(patha)\n",
        "#model1.load_state_dict(trained)\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/gradable_folder1'\n",
        "lstdir=os.listdir(path)\n",
        "predict111=[]\n",
        "slice_scores=[]\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(20, 20), dpi=80)\n",
        "for i in lstdir:\n",
        "  print(i)\n",
        "  pathim=os.path.join(path,i)\n",
        "  input_image_array=cv2.imread(pathim)\n",
        "  input_image_array= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "  input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "  print(input_image_array.shape)\n",
        "  np_img=input_image_array\n",
        "  torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "  torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "  torch_array=torch_array.float().to(device)\n",
        "  ####### load pytorch model for prediction\n",
        "  with torch.no_grad():\n",
        "    model1=model1.to(device)\n",
        "    torch_array=torch_array.to(device)\n",
        "    batch_rec = model1(torch_array)\n",
        "    loss = torch.mean(torch.pow(torch_array - batch_rec, 2), dim=(1, 2, 3))\n",
        "    slice_scores += loss.cpu().tolist()\n",
        "    batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()\n",
        "    #slice_scores.append()\n",
        "    #import matplotlib.pyplot as plt\n",
        "    #plt.subplot(1,41,i+1)\n",
        "    #plt.title(x.split('.')[0])\n",
        "    #plt.axis('off')\n",
        "    #plt.figure(figsize=(4, 4)) \n",
        "    #plt.imshow(img)\n",
        "    #plt.imshow(batch_rec1[1,:,:])\n",
        "    #plt.figure(figsize=(4, 4)) \n",
        "    #figure(figsize=(50, 50))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "iZrpSGr3d86B",
        "outputId": "025a6d5b-6f95-4f6f-8406-6e6aadd36fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN000000.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000001.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000002.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000034.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000060.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000068.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000100.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000106.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000122.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000127.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000140.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000188.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000195.jpg\n",
            "(128, 128, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slice_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV3NZ-f4feQG",
        "outputId": "c6484f5a-bbff-4b5a-efd6-6440815e9d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012163090519607067,\n",
              " 0.009697746485471725,\n",
              " 0.005191497039049864,\n",
              " 0.002611537929624319,\n",
              " 0.007952315732836723,\n",
              " 0.005985564552247524,\n",
              " 0.012930388562381268,\n",
              " 0.021622350439429283,\n",
              " 0.010297257453203201,\n",
              " 0.005026031285524368,\n",
              " 0.022304711863398552,\n",
              " 0.013309127651154995,\n",
              " 0.009648850187659264]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Irgos_challenege2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sel1r6h5go1t",
        "outputId": "4fda56a4-f041-467a-ed4b-297906b49fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Irgos_challenege2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Irgos_challenege2022/Glaucoma_Positive.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rLSr6NCngiPH",
        "outputId": "719c63bf-475a-4a28-c592-31c7715c0030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Irgos_challenege2022/Glaucoma_Positive.zip\n",
            "   creating: Glaucoma_Positive/\n",
            "  inflating: Glaucoma_Positive/598.jpg  \n",
            "  inflating: Glaucoma_Positive/600.jpg  \n",
            "  inflating: Glaucoma_Positive/601.jpg  \n",
            "  inflating: Glaucoma_Positive/604.jpg  \n",
            "  inflating: Glaucoma_Positive/605.jpg  \n",
            "  inflating: Glaucoma_Positive/608.jpg  \n",
            "  inflating: Glaucoma_Positive/609.jpg  \n",
            "  inflating: Glaucoma_Positive/612.jpg  \n",
            "  inflating: Glaucoma_Positive/613.jpg  \n",
            "  inflating: Glaucoma_Positive/615.jpg  \n",
            "  inflating: Glaucoma_Positive/617.jpg  \n",
            "  inflating: Glaucoma_Positive/618.jpg  \n",
            "  inflating: Glaucoma_Positive/619.jpg  \n",
            "  inflating: Glaucoma_Positive/621.jpg  \n",
            "  inflating: Glaucoma_Positive/622.jpg  \n",
            "  inflating: Glaucoma_Positive/623.jpg  \n",
            "  inflating: Glaucoma_Positive/624.jpg  \n",
            "  inflating: Glaucoma_Positive/626.jpg  \n",
            "  inflating: Glaucoma_Positive/627.jpg  \n",
            "  inflating: Glaucoma_Positive/628.jpg  \n",
            "  inflating: Glaucoma_Positive/630.jpg  \n",
            "  inflating: Glaucoma_Positive/631.jpg  \n",
            "  inflating: Glaucoma_Positive/632.jpg  \n",
            "  inflating: Glaucoma_Positive/634.jpg  \n",
            "  inflating: Glaucoma_Positive/636.jpg  \n",
            "  inflating: Glaucoma_Positive/638.jpg  \n",
            "  inflating: Glaucoma_Positive/640.jpg  \n",
            "  inflating: Glaucoma_Positive/641.jpg  \n",
            "  inflating: Glaucoma_Positive/644.jpg  \n",
            "  inflating: Glaucoma_Positive/645.jpg  \n",
            "  inflating: Glaucoma_Positive/646.jpg  \n",
            "  inflating: Glaucoma_Positive/647.jpg  \n",
            "  inflating: Glaucoma_Positive/648.jpg  \n",
            "  inflating: Glaucoma_Positive/650.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/Glaucoma_Positive'\n",
        "lstdir=os.listdir(path)\n",
        "predict111=[]\n",
        "slice_scores=[]\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(20, 20), dpi=80)\n",
        "for i in lstdir:\n",
        "  print(i)\n",
        "  pathim=os.path.join(path,i)\n",
        "  input_image_array=cv2.imread(pathim)\n",
        "  input_image_array= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "  input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "  print(input_image_array.shape)\n",
        "  np_img=input_image_array\n",
        "  torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "  torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "  torch_array=torch_array.float().to(device)\n",
        "  ####### load pytorch model for prediction\n",
        "  with torch.no_grad():\n",
        "    model1=model1.to(device)\n",
        "    torch_array=torch_array.to(device)\n",
        "    batch_rec = model1(torch_array)\n",
        "    loss = torch.mean(torch.pow(torch_array - batch_rec, 2), dim=(1, 2, 3))\n",
        "    slice_scores += loss.cpu().tolist()\n",
        "    batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iew9Pg1Cg0O8",
        "outputId": "68a1df31-6803-4afa-a7ad-31d42d5f2971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "598.jpg\n",
            "(128, 128, 3)\n",
            "600.jpg\n",
            "(128, 128, 3)\n",
            "601.jpg\n",
            "(128, 128, 3)\n",
            "604.jpg\n",
            "(128, 128, 3)\n",
            "605.jpg\n",
            "(128, 128, 3)\n",
            "608.jpg\n",
            "(128, 128, 3)\n",
            "609.jpg\n",
            "(128, 128, 3)\n",
            "612.jpg\n",
            "(128, 128, 3)\n",
            "613.jpg\n",
            "(128, 128, 3)\n",
            "615.jpg\n",
            "(128, 128, 3)\n",
            "617.jpg\n",
            "(128, 128, 3)\n",
            "618.jpg\n",
            "(128, 128, 3)\n",
            "619.jpg\n",
            "(128, 128, 3)\n",
            "621.jpg\n",
            "(128, 128, 3)\n",
            "622.jpg\n",
            "(128, 128, 3)\n",
            "623.jpg\n",
            "(128, 128, 3)\n",
            "624.jpg\n",
            "(128, 128, 3)\n",
            "626.jpg\n",
            "(128, 128, 3)\n",
            "627.jpg\n",
            "(128, 128, 3)\n",
            "628.jpg\n",
            "(128, 128, 3)\n",
            "630.jpg\n",
            "(128, 128, 3)\n",
            "631.jpg\n",
            "(128, 128, 3)\n",
            "632.jpg\n",
            "(128, 128, 3)\n",
            "634.jpg\n",
            "(128, 128, 3)\n",
            "636.jpg\n",
            "(128, 128, 3)\n",
            "638.jpg\n",
            "(128, 128, 3)\n",
            "640.jpg\n",
            "(128, 128, 3)\n",
            "641.jpg\n",
            "(128, 128, 3)\n",
            "644.jpg\n",
            "(128, 128, 3)\n",
            "645.jpg\n",
            "(128, 128, 3)\n",
            "646.jpg\n",
            "(128, 128, 3)\n",
            "647.jpg\n",
            "(128, 128, 3)\n",
            "648.jpg\n",
            "(128, 128, 3)\n",
            "650.jpg\n",
            "(128, 128, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slice_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8Og_sE4g_vb",
        "outputId": "705d68c6-c053-401c-fd64-b9a29a85e335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.01804514229297638,\n",
              " 0.009771816432476044,\n",
              " 0.025158746168017387,\n",
              " 0.01697489060461521,\n",
              " 0.017387622967362404,\n",
              " 0.015255950391292572,\n",
              " 0.024175401777029037,\n",
              " 0.009415493346750736,\n",
              " 0.021733563393354416,\n",
              " 0.012733248993754387,\n",
              " 0.014180898666381836,\n",
              " 0.016821663826704025,\n",
              " 0.015200376510620117,\n",
              " 0.023219680413603783,\n",
              " 0.0282975435256958,\n",
              " 0.014606358483433723,\n",
              " 0.01406135968863964,\n",
              " 0.017964094877243042,\n",
              " 0.022680586203932762,\n",
              " 0.02405652031302452,\n",
              " 0.02681424841284752,\n",
              " 0.024043235927820206,\n",
              " 0.02157694660127163,\n",
              " 0.033674456179142,\n",
              " 0.016538910567760468,\n",
              " 0.018103256821632385,\n",
              " 0.025489207357168198,\n",
              " 0.023399382829666138,\n",
              " 0.013156796805560589,\n",
              " 0.021898292005062103,\n",
              " 0.021493209525942802,\n",
              " 0.014711610972881317,\n",
              " 0.024420421570539474,\n",
              " 0.02451167069375515]"
            ]
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/Irgos_challenege2022/ImagesACRIMA.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBAW6kFxhLTF",
        "outputId": "7dd6e6a5-ec53-4c9b-f5ba-3f2603b6e013"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/Irgos_challenege2022/ImagesACRIMA.zip\n",
            "   creating: ImagesACRIMA/\n",
            "  inflating: ImagesACRIMA/Im001_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im002_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im003_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im004_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im005_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im006_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im007_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im008_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im009_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im010_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im011_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im012_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im013_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im014_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im015_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im016_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im017_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im018_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im019_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im020_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im021_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im022_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im023_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im024_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im025_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im026_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im027_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im028_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im029_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im030_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im031_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im032_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im033_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im034_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im035_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im036_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im037_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im038_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im039_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im040_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im041_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im042_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im043_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im044_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im045_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im046_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im047_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im048_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im049_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im050_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im051_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im052_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im053_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im054_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im055_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im056_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im057_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im058_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im059_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im060_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im061_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im062_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im063_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im064_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im065_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im066_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im067_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im068_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im069_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im070_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im071_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im072_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im073_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im074_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im075_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im076_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im077_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im078_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im079_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im080_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im081_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im082_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im083_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im084_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im085_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im086_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im087_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im088_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im089_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im090_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im091_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im092_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im093_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im094_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im095_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im096_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im097_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im098_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im099_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im100_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im101_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im102_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im103_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im104_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im105_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im106_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im107_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im108_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im109_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im110_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im111_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im112_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im113_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im114_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im115_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im116_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im117_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im118_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im119_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im120_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im121_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im122_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im123_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im124_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im125_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im126_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im127_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im128_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im129_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im130_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im131_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im132_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im133_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im134_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im135_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im136_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im137_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im138_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im139_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im140_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im141_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im142_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im143_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im144_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im145_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im146_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im147_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im148_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im149_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im150_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im151_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im152_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im153_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im154_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im155_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im156_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im157_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im158_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im159_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im160_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im161_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im162_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im163_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im164_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im165_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im166_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im167_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im168_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im169_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im170_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im171_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im172_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im173_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im174_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im175_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im176_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im177_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im178_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im179_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im180_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im181_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im182_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im183_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im184_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im185_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im186_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im187_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im188_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im189_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im190_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im191_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im192_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im193_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im194_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im195_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im196_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im197_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im198_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im199_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im200_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im201_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im202_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im203_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im204_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im205_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im206_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im207_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im208_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im209_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im210_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im211_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im212_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im213_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im214_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im215_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im216_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im217_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im218_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im219_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im220_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im221_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im222_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im223_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im224_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im225_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im226_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im227_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im228_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im229_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im230_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im231_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im232_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im233_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im234_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im235_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im236_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im237_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im238_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im239_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im240_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im241_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im242_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im243_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im244_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im245_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im246_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im247_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im248_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im249_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im250_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im251_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im252_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im253_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im254_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im255_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im256_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im257_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im258_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im259_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im260_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im261_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im262_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im263_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im264_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im265_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im266_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im267_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im268_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im269_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im270_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im271_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im272_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im273_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im274_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im275_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im276_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im277_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im278_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im279_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im280_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im281_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im282_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im283_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im284_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im285_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im286_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im287_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im288_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im289_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im290_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im291_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im292_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im293_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im294_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im295_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im296_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im297_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im298_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im299_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im300_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im301_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im302_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im303_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im304_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im305_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im306_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im307_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im308_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im309_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im310_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im311_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im312_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im313_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im314_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im315_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im316_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im317_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im318_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im319_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im320_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im321_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im322_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im323_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im324_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im325_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im326_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im327_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im328_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im329_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im330_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im331_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im332_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im333_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im334_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im335_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im336_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im337_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im338_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im339_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im340_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im341_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im342_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im343_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im344_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im345_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im346_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im347_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im348_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im349_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im350_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im351_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im352_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im353_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im354_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im355_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im356_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im357_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im358_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im359_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im360_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im361_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im362_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im363_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im364_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im365_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im366_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im367_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im368_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im369_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im370_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im371_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im372_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im373_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im374_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im375_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im376_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im377_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im378_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im379_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im380_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im381_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im382_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im383_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im384_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im385_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im386_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im387_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im388_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im389_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im390_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im391_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im392_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im393_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im394_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im395_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im396_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im397_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im398_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im399_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im400_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im401_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im402_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im403_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im404_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im405_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im406_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im407_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im408_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im409_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im410_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im411_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im412_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im413_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im414_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im415_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im416_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im417_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im418_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im419_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im420_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im421_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im422_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im423_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im424_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im425_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im426_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im427_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im428_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im429_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im430_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im431_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im432_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im433_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im434_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im435_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im436_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im437_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im438_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im439_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im440_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im441_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im442_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im443_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im444_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im445_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im446_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im447_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im448_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im449_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im450_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im451_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im452_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im453_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im454_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im455_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im456_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im457_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im458_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im459_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im460_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im461_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im462_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im463_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im464_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im465_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im466_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im467_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im468_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im469_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im470_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im471_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im472_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im473_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im474_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im475_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im476_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im477_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im478_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im479_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im480_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im481_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im482_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im483_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im484_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im485_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im486_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im487_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im488_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im489_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im490_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im491_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im492_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im493_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im494_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im495_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im496_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im497_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im498_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im499_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im500_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im501_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im502_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im503_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im504_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im505_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im506_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im507_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im508_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im509_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im510_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im511_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im512_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im513_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im514_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im515_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im516_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im517_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im518_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im519_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im520_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im521_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im522_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im523_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im524_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im525_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im526_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im527_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im528_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im529_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im530_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im531_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im532_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im533_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im534_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im535_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im536_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im537_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im538_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im539_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im540_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im541_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im542_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im543_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im544_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im545_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im546_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im547_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im548_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im549_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im550_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im551_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im552_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im553_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im554_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im555_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im556_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im557_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im558_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im559_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im560_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im561_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im562_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im563_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im564_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im565_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im566_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im567_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im568_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im569_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im570_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im571_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im572_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im573_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im574_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im575_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im576_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im577_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im578_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im579_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im580_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im581_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im582_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im583_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im584_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im585_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im586_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im587_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im588_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im589_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im590_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im591_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im592_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im593_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im594_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im595_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im596_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im597_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im598_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im599_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im600_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im601_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im602_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im603_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im604_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im605_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im606_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im607_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im608_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im609_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im610_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im611_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im612_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im613_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im614_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im615_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im616_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im617_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im618_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im619_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im620_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im621_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im622_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im623_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im624_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im625_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im626_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im627_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im628_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im629_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im630_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im631_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im632_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im633_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im634_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im635_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im636_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im637_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im638_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im639_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im640_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im641_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im642_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im643_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im644_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im645_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im646_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im647_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im648_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im649_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im650_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im651_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im652_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im653_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im654_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im655_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im656_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im657_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im658_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im659_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im660_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im661_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im662_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im663_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im664_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im665_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im666_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im667_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im668_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im669_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im670_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im671_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im672_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im673_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im674_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im675_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im676_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im677_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im678_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im679_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im680_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im681_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im682_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im683_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im684_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im685_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im686_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im687_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im688_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im689_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im690_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im691_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im692_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im693_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im694_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im695_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im696_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im697_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im698_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im699_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im700_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im701_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im702_g_ACRIMA.JPG  \n",
            "  inflating: ImagesACRIMA/Im703_g_ACRIMA.JPG  \n",
            "  inflating: ImagesACRIMA/Im704_g_ACRIMA.jpg  \n",
            "  inflating: ImagesACRIMA/Im705_g_ACRIMA.jpg  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/gradable_folder1'\n",
        "lstdir=os.listdir(path)\n",
        "predict111=[]\n",
        "slice_scores=[]\n",
        "from matplotlib.pyplot import figure\n",
        "figure(figsize=(20, 20), dpi=80)\n",
        "for i in lstdir:\n",
        "  print(i)\n",
        "  pathim=os.path.join(path,i)\n",
        "  input_image_array=cv2.imread(pathim)\n",
        "  input_image_array= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "  input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "  print(input_image_array.shape)\n",
        "  np_img=input_image_array\n",
        "  torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "  torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "  torch_array=torch_array.float().to(device)\n",
        "  ####### load pytorch model for prediction\n",
        "  with torch.no_grad():\n",
        "    model1=model1.to(device)\n",
        "    torch_array=torch_array.to(device)\n",
        "    batch_rec = model1(torch_array)\n",
        "    loss = torch.mean(torch.pow(torch_array - batch_rec, 2), dim=(1, 2, 3))\n",
        "    slice_scores += loss.cpu().tolist()\n",
        "    batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()\n",
        "#pred = float(np.array(slice_scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "O0GBfvecheJE",
        "outputId": "34d93319-0ab6-43a4-e9bf-0b49f10f9bbf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TRAIN000000.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000001.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000002.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000034.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000060.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000068.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000100.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000106.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000122.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000127.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000140.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000188.jpg\n",
            "(128, 128, 3)\n",
            "TRAIN000195.jpg\n",
            "(128, 128, 3)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1600x1600 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(batch_rec1[1,:,:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "id": "QfAv3AFsh627",
        "outputId": "ea8b45c3-7aef-4fb0-b457-33a22c90a89c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f9862c58150>"
            ]
          },
          "metadata": {},
          "execution_count": 149
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "slice_scores"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4fONAgZkIQa",
        "outputId": "336c250e-04ca-45df-cbed-4914405cdd64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.012163090519607067,\n",
              " 0.009697746485471725,\n",
              " 0.005191497039049864,\n",
              " 0.002611537929624319,\n",
              " 0.007952315732836723,\n",
              " 0.005985564552247524,\n",
              " 0.012930388562381268,\n",
              " 0.021622350439429283,\n",
              " 0.010297257453203201,\n",
              " 0.005026031285524368,\n",
              " 0.022304711863398552,\n",
              " 0.013309127651154995,\n",
              " 0.009648850187659264]"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "pred = np.array(slice_scores).astype(float)"
      ],
      "metadata": {
        "id": "03HdQQPGhyB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDJA6MmhmPc5",
        "outputId": "7e5636cd-1cb1-438f-e5e0-84089c76d1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.01216309, 0.00969775, 0.0051915 , 0.00261154, 0.00795232,\n",
              "       0.00598556, 0.01293039, 0.02162235, 0.01029726, 0.00502603,\n",
              "       0.02230471, 0.01330913, 0.00964885])"
            ]
          },
          "metadata": {},
          "execution_count": 152
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pred1 = np.clip(pred, a_max=1.0, a_min=0.0)\n",
        "#pred1"
      ],
      "metadata": {
        "id": "-4yyrs14ky6t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred2=[pred>0.1]"
      ],
      "metadata": {
        "id": "QFmw9t22io_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdJut2QLlIYB",
        "outputId": "bf3a1480-ac11-453c-c70e-539d22271a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([False, False, False, False, False, False, False, False, False,\n",
              "        False, False, False, False])]"
            ]
          },
          "metadata": {},
          "execution_count": 155
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "y1AA94pjpENO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hT2pEHVCpER0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "from evalutils.io import ImageLoader\n",
        "\n",
        "\n",
        "class DummyLoader(ImageLoader):\n",
        "    @staticmethod\n",
        "    def load_image(fname):\n",
        "        return str(fname)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def hash_image(image):\n",
        "        return hash(image)\n",
        "\n",
        "\n",
        "class airogs_algorithm(ClassificationAlgorithm):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            validators=dict(\n",
        "                input_image=(\n",
        "                    UniqueImagesValidator(),\n",
        "                    UniquePathIndicesValidator(),\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self._file_loaders = dict(input_image=DummyLoader())\n",
        "\n",
        "        self.output_keys = [\"multiple-referable-glaucoma-likelihoods\", \n",
        "                            \"multiple-referable-glaucoma-binary\",\n",
        "                            \"multiple-ungradability-scores\",\n",
        "                            \"multiple-ungradability-binary\"]\n",
        "    \n",
        "    def load(self):\n",
        "        for key, file_loader in self._file_loaders.items():\n",
        "            fltr = (\n",
        "                self._file_filters[key] if key in self._file_filters else None\n",
        "            )\n",
        "            self._cases[key] = self._load_cases(\n",
        "                folder=Path(\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/\"),\n",
        "                file_loader=file_loader,\n",
        "                file_filter=fltr,\n",
        "            )\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def combine_dicts(self, dicts):\n",
        "        out = {}\n",
        "        for d in dicts:\n",
        "            for k, v in d.items():\n",
        "                if k not in out:\n",
        "                    out[k] = []\n",
        "                out[k].append(v)\n",
        "        return out\n",
        "    \n",
        "path=\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/phase_1.tiff\"\n",
        "# Load and test the image(s) for this case\n",
        "likelihood1=[]\n",
        "rg_binary1=[]\n",
        "ungradability_score1=[]\n",
        "ungradability_binary1=[]\n",
        "with tifffile.TiffFile(path) as stack:\n",
        "  #predict11=[]\n",
        "  #slice_scores=[]\n",
        "  #predic00=[]\n",
        "  for page in stack.pages:\n",
        "    input_image_array = page.asarray()\n",
        "    #print(input_image_array.shape)\n",
        "    input_image_array= cv2.resize(input_image_array, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "    input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "    print(input_image_array.shape)\n",
        "    np_img=input_image_array\n",
        "    torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "    torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "    torch_array=torch_array.float().to(device)\n",
        "    ####### load pytorch model for prediction\n",
        "    model=CNN()\n",
        "    model=nn.DataParallel(model)\n",
        "    model=model.to(device)\n",
        "    pretrained=torch.load(path1)\n",
        "    #model=nn.DataParallel(model)\n",
        "    #model=model.to(device)\n",
        "    model.load_state_dict(pretrained)\n",
        "    with torch.no_grad():\n",
        "      y_pred,_=model(torch_array)\n",
        "      pc1= torch.softmax(y_pred, dim=1)\n",
        "      pc1=pc1.squeeze(axis=0)\n",
        "      predic0,predic1=pc1.detach().cpu().numpy()\n",
        "      #p2=predic1\n",
        "      #predict11.append(p2)\n",
        "      #predic00.append(predic0)\n",
        "      input_image_array1= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "      input_image_array1=((input_image_array1 - input_image_array1.min()) / (input_image_array1.max() - input_image_array1.min()))\n",
        "      print(input_image_array1.shape)\n",
        "      np_img1=input_image_array1\n",
        "      torch_array1=torch.from_numpy(np_img1).unsqueeze(axis=0)\n",
        "      torch_array1=torch_array1.permute(0, 3, 1, 2)\n",
        "      torch_array1=torch_array1.float().to(device)\n",
        "      model1=model1.to(device)\n",
        "      torch_array1=torch_array1.to(device)\n",
        "      batch_rec = model1(torch_array1)\n",
        "      loss = torch.mean(torch.pow(torch_array1 - batch_rec, 2), dim=(1, 2, 3))\n",
        "      slice_scores += loss.cpu().tolist()\n",
        "      #batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()\n",
        "      ###### output probabilties computed here\n",
        "      likelihood=predic1\n",
        "      rg_binary=(likelihood>0.5)\n",
        "      un_gradable=predic0\n",
        "      predb0=(un_gradable>0.5)\n",
        "      ungradability_score=loss.cpu().numpy()*predb0\n",
        "      ungradability_score=np.squeeze(ungradability_score)\n",
        "      ungradability_binary=ungradability_score>0\n",
        "\n",
        "      likelihood1.append(likelihood)\n",
        "      rg_binary1.append(rg_binary)\n",
        "      ungradability_score1.append(ungradability_score)\n",
        "      ungradability_binary1.append(ungradability_binary)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5njlrjEXpEUj",
        "outputId": "d1150dac-e080-4ad9-b978-4f183fe3952d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n",
            "(256, 256, 3)\n",
            "(128, 128, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ungradability_binary1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsoD7S3Rx6Fl",
        "outputId": "8a425163-ae41-4818-b9c5-d92bbeb9d63c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[True, True, True, False, True, False, True, True, True, True]"
            ]
          },
          "metadata": {},
          "execution_count": 220
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.array(ungradability_score1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoJQIh95x21e",
        "outputId": "b779c406-90bf-4a92-a4b9-e326d19bc29c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0041809753"
            ]
          },
          "metadata": {},
          "execution_count": 223
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rg_binary1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NGvu6AEYxzRV",
        "outputId": "25f0a220-cd91-4a22-b3c5-08a2cedf6b96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[False, False, False, True, False, True, False, False, False, False]"
            ]
          },
          "metadata": {},
          "execution_count": 207
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "likelihood1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Agg_-_g1xuH2",
        "outputId": "ad7ebec0-7830-40d0-a514-9bc6ca6773b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[4.75302e-05,\n",
              " 4.4433564e-05,\n",
              " 5.8574675e-05,\n",
              " 0.9977761,\n",
              " 4.4186116e-05,\n",
              " 0.9994217,\n",
              " 4.3568405e-05,\n",
              " 4.198575e-05,\n",
              " 4.734292e-05,\n",
              " 4.4428223e-05]"
            ]
          },
          "metadata": {},
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_array=np.array(predict11)\n",
        "print(np_array)\n",
        "predb=np_array>0.5\n",
        "predb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he_r2LwoqQ4t",
        "outputId": "7535ab05-144f-4f78-a14f-170494e67d13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[4.7530200e-05 4.4433564e-05 5.8574675e-05 9.9777609e-01 4.4186116e-05\n",
            " 9.9942172e-01 4.3568405e-05 4.1985750e-05 4.7342921e-05 4.4428223e-05]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([False, False, False,  True, False,  True, False, False, False,\n",
              "       False])"
            ]
          },
          "metadata": {},
          "execution_count": 186
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_array0=np.array(predic00)\n",
        "predb0=(np_array0>0.5)\n",
        "predb0\n",
        "#np_array0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7pUX1RzrjA1",
        "outputId": "cd293839-6c7a-4704-f1b3-5b025ade540c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {},
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np_array0=np.array(predic00)\n",
        "predb0=(np_array0>0.5)\n",
        "d1=slice_scores*predb0\n",
        "d2=d1>0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "id": "ZyZqxps6sHwN",
        "outputId": "087553b8-0bbe-4c53-da37-7601daeaa0c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-60344668c225>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpredb0\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_array0\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0md1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice_scores\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpredb0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0md2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md1\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YajklgSvdgw",
        "outputId": "158135e7-7450-4e42-deac-1bd7479e07bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "d1=slice_scores*predb0\n",
        "d2=d1>0\n",
        "d2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuGPPoGyqCyu",
        "outputId": "5c81ed95-9bd1-4111-f0f4-ad985e6d66de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True, False,  True, False,  True,  True,  True,\n",
              "        True])"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    }
  ]
}