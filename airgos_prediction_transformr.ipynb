{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "airgos_prediction_transformr.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHs1j7H2Uyk-",
        "outputId": "20f3c32e-b4e4-4b13-ea46-b0e1321dc294"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZCxHtB_bVCYg",
        "outputId": "e951e8e5-e211-43fe-ae01-324dc1a2c589"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Irgos_challenege2022"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eNKhTo7VO8g",
        "outputId": "61c02b1c-93d8-408f-e1e5-50dfde2e7948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Irgos_challenege2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm\n",
        "!pip install evalutils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raUfzy7DVV_g",
        "outputId": "a67fb37a-593a-45a9-9691-152aeb805ac1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting timm\n",
            "  Downloading timm-0.5.4-py3-none-any.whl (431 kB)\n",
            "\u001b[?25l\r\u001b[K     |▊                               | 10 kB 16.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 20 kB 12.3 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 30 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 40 kB 8.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 61 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |██████                          | 81 kB 5.9 MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 92 kB 6.5 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 102 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 112 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 122 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 133 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 143 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 153 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 163 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 174 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 184 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 194 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 204 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 215 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 225 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 235 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 245 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 256 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 266 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 276 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 286 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 296 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 307 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 317 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 327 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 337 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 348 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 358 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 368 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 378 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 389 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 399 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 409 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 419 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 430 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 431 kB 5.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.21.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.5.4\n",
            "Collecting evalutils\n",
            "  Downloading evalutils-0.3.1-py3-none-any.whl (558 kB)\n",
            "\u001b[K     |████████████████████████████████| 558 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.0.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from evalutils) (2.4.1)\n",
            "Requirement already satisfied: pandas!=0.24.0 in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.3.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from evalutils) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from evalutils) (1.21.5)\n",
            "Collecting cookiecutter\n",
            "  Downloading cookiecutter-1.7.3-py2.py3-none-any.whl (34 kB)\n",
            "Collecting SimpleITK\n",
            "  Downloading SimpleITK-2.1.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (48.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 48.4 MB 2.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=0.24.0->evalutils) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas!=0.24.0->evalutils) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas!=0.24.0->evalutils) (1.15.0)\n",
            "Requirement already satisfied: python-slugify>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (6.0.1)\n",
            "Collecting binaryornot>=0.4.4\n",
            "  Downloading binaryornot-0.4.4-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting poyo>=0.5.0\n",
            "  Downloading poyo-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: Jinja2<4.0.0,>=2.7 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (2.11.3)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.7/dist-packages (from cookiecutter->evalutils) (2.23.0)\n",
            "Collecting jinja2-time>=0.2.0\n",
            "  Downloading jinja2_time-0.2.0-py2.py3-none-any.whl (6.4 kB)\n",
            "Requirement already satisfied: chardet>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from binaryornot>=0.4.4->cookiecutter->evalutils) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2<4.0.0,>=2.7->cookiecutter->evalutils) (2.0.1)\n",
            "Collecting arrow\n",
            "  Downloading arrow-1.2.2-py3-none-any.whl (64 kB)\n",
            "\u001b[K     |████████████████████████████████| 64 kB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.7/dist-packages (from python-slugify>=4.0.0->cookiecutter->evalutils) (1.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.23.0->cookiecutter->evalutils) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from arrow->jinja2-time>=0.2.0->cookiecutter->evalutils) (3.10.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from imageio->evalutils) (7.1.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evalutils) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->evalutils) (3.1.0)\n",
            "Installing collected packages: arrow, poyo, jinja2-time, binaryornot, SimpleITK, cookiecutter, evalutils\n",
            "Successfully installed SimpleITK-2.1.1 arrow-1.2.2 binaryornot-0.4.4 cookiecutter-1.7.3 evalutils-0.3.1 jinja2-time-0.2.0 poyo-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imagecodecs==2021.11.20"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZ3GZKEfVY_A",
        "outputId": "5a6f1bb0-b405-43e7-efbd-2d74db3f66fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting imagecodecs==2021.11.20\n",
            "  Downloading imagecodecs-2021.11.20-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (31.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 31.0 MB 2.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.7/dist-packages (from imagecodecs==2021.11.20) (1.21.5)\n",
            "Installing collected packages: imagecodecs\n",
            "Successfully installed imagecodecs-2021.11.20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tifffile==2021.8.8"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X2LPTrN4Vb4v",
        "outputId": "e8a23bb3-52b1-4479-b99c-7d97d144f8e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tifffile==2021.8.8\n",
            "  Downloading tifffile-2021.8.8-py3-none-any.whl (171 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 17.7 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 20 kB 23.7 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 30 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 61 kB 5.3 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 71 kB 5.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 81 kB 4.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 92 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 102 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 112 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 122 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 133 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 143 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 153 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 163 kB 5.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 171 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tifffile==2021.8.8) (1.21.5)\n",
            "Installing collected packages: tifffile\n",
            "  Attempting uninstall: tifffile\n",
            "    Found existing installation: tifffile 2021.11.2\n",
            "    Uninstalling tifffile-2021.11.2:\n",
            "      Successfully uninstalled tifffile-2021.11.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed tifffile-2021.8.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ttach"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lz2DOq5usFfE",
        "outputId": "cb5633ab-c4dc-4a00-c5fc-07edfd43cf20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from traitlets.traitlets import Bool\n",
        "from typing import Dict\n",
        "\n",
        "import SimpleITK\n",
        "import tqdm\n",
        "import json\n",
        "from pathlib import Path\n",
        "import tifffile\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "path='/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/2dmodelairgos.pth'\n",
        "\n",
        "########### define model object for prediction ########\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from timm.models.layers import trunc_normal_, DropPath\n",
        "from timm.models.registry import register_model\n",
        "\n",
        "class Block(nn.Module):\n",
        "    r\"\"\" ConvNeXt Block. There are two equivalent implementations:\n",
        "    (1) DwConv -> LayerNorm (channels_first) -> 1x1 Conv -> GELU -> 1x1 Conv; all in (N, C, H, W)\n",
        "    (2) DwConv -> Permute to (N, H, W, C); LayerNorm (channels_last) -> Linear -> GELU -> Linear; Permute back\n",
        "    We use (2) as we find it slightly faster in PyTorch\n",
        "    \n",
        "    Args:\n",
        "        dim (int): Number of input channels.\n",
        "        drop_path (float): Stochastic depth rate. Default: 0.0\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim, drop_path=0., layer_scale_init_value=1e-6):\n",
        "        super().__init__()\n",
        "        self.dwconv = nn.Conv2d(dim, dim, kernel_size=7, padding=3, groups=dim) # depthwise conv\n",
        "        self.norm = LayerNorm(dim, eps=1e-6)\n",
        "        self.pwconv1 = nn.Linear(dim, 4 * dim) # pointwise/1x1 convs, implemented with linear layers\n",
        "        self.act = nn.GELU()\n",
        "        self.pwconv2 = nn.Linear(4 * dim, dim)\n",
        "        self.gamma = nn.Parameter(layer_scale_init_value * torch.ones((dim)), \n",
        "                                    requires_grad=True) if layer_scale_init_value > 0 else None\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        input = x\n",
        "        x = self.dwconv(x)\n",
        "        x = x.permute(0, 2, 3, 1) # (N, C, H, W) -> (N, H, W, C)\n",
        "        x = self.norm(x)\n",
        "        x = self.pwconv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.pwconv2(x)\n",
        "        if self.gamma is not None:\n",
        "            x = self.gamma * x\n",
        "        x = x.permute(0, 3, 1, 2) # (N, H, W, C) -> (N, C, H, W)\n",
        "\n",
        "        x = input + self.drop_path(x)\n",
        "        return x\n",
        "\n",
        "class ConvNeXt(nn.Module):\n",
        "    r\"\"\" ConvNeXt\n",
        "        A PyTorch impl of : `A ConvNet for the 2020s`  -\n",
        "          https://arxiv.org/pdf/2201.03545.pdf\n",
        "    Args:\n",
        "        in_chans (int): Number of input image channels. Default: 3\n",
        "        num_classes (int): Number of classes for classification head. Default: 1000\n",
        "        depths (tuple(int)): Number of blocks at each stage. Default: [3, 3, 9, 3]\n",
        "        dims (int): Feature dimension at each stage. Default: [96, 192, 384, 768]\n",
        "        drop_path_rate (float): Stochastic depth rate. Default: 0.\n",
        "        layer_scale_init_value (float): Init value for Layer Scale. Default: 1e-6.\n",
        "        head_init_scale (float): Init scaling value for classifier weights and biases. Default: 1.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_chans=3, num_classes=1000, \n",
        "                 depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], drop_path_rate=0., \n",
        "                 layer_scale_init_value=1e-6, head_init_scale=1.,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.downsample_layers = nn.ModuleList() # stem and 3 intermediate downsampling conv layers\n",
        "        stem = nn.Sequential(\n",
        "            nn.Conv2d(in_chans, dims[0], kernel_size=4, stride=4),\n",
        "            LayerNorm(dims[0], eps=1e-6, data_format=\"channels_first\")\n",
        "        )\n",
        "        self.downsample_layers.append(stem)\n",
        "        for i in range(3):\n",
        "            downsample_layer = nn.Sequential(\n",
        "                    LayerNorm(dims[i], eps=1e-6, data_format=\"channels_first\"),\n",
        "                    nn.Conv2d(dims[i], dims[i+1], kernel_size=2, stride=2),\n",
        "            )\n",
        "            self.downsample_layers.append(downsample_layer)\n",
        "\n",
        "        self.stages = nn.ModuleList() # 4 feature resolution stages, each consisting of multiple residual blocks\n",
        "        dp_rates=[x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))] \n",
        "        cur = 0\n",
        "        for i in range(4):\n",
        "            stage = nn.Sequential(\n",
        "                *[Block(dim=dims[i], drop_path=dp_rates[cur + j], \n",
        "                layer_scale_init_value=layer_scale_init_value) for j in range(depths[i])]\n",
        "            )\n",
        "            self.stages.append(stage)\n",
        "            cur += depths[i]\n",
        "\n",
        "        self.norm = nn.LayerNorm(dims[-1], eps=1e-6) # final norm layer\n",
        "        self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "        self.head.weight.data.mul_(head_init_scale)\n",
        "        self.head.bias.data.mul_(head_init_scale)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward_features(self, x):\n",
        "        for i in range(4):\n",
        "            x = self.downsample_layers[i](x)\n",
        "            x = self.stages[i](x)\n",
        "        return self.norm(x.mean([-2, -1])) # global average pooling, (N, C, H, W) -> (N, C)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_features(x)\n",
        "        out=x\n",
        "        #print(x1.shape)\n",
        "        x = self.head(x)\n",
        "        return x,out\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    r\"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
        "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
        "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
        "    with shape (batch_size, channels, height, width).\n",
        "    \"\"\"\n",
        "    def __init__(self, normalized_shape, eps=1e-6, data_format=\"channels_last\"):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.eps = eps\n",
        "        self.data_format = data_format\n",
        "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
        "            raise NotImplementedError \n",
        "        self.normalized_shape = (normalized_shape, )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        if self.data_format == \"channels_last\":\n",
        "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
        "        elif self.data_format == \"channels_first\":\n",
        "            u = x.mean(1, keepdim=True)\n",
        "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
        "            x = (x - u) / torch.sqrt(s + self.eps)\n",
        "            x = self.weight[:, None, None] * x + self.bias[:, None, None]\n",
        "            return x\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    \"convnext_tiny_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_tiny_1k_224_ema.pth\",\n",
        "    \"convnext_small_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_small_1k_224_ema.pth\",\n",
        "    \"convnext_base_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_1k_224_ema.pth\",\n",
        "    \"convnext_large_1k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_1k_224_ema.pth\",\n",
        "    \"convnext_base_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_base_22k_224.pth\",\n",
        "    \"convnext_large_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_large_22k_224.pth\",\n",
        "    \"convnext_xlarge_22k\": \"https://dl.fbaipublicfiles.com/convnext/convnext_xlarge_22k_224.pth\",\n",
        "}\n",
        "\n",
        "@register_model\n",
        "def convnext_tiny(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 9, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_tiny_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\", check_hash=True)\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_small(pretrained=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[96, 192, 384, 768], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_small_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_base(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[128, 256, 512, 1024], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_base_22k'] if in_22k else model_urls['convnext_base_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_large(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[192, 384, 768, 1536], **kwargs)\n",
        "    if pretrained:\n",
        "        url = model_urls['convnext_large_22k'] if in_22k else model_urls['convnext_large_1k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "\n",
        "@register_model\n",
        "def convnext_xlarge(pretrained=False, in_22k=False, **kwargs):\n",
        "    model = ConvNeXt(depths=[3, 3, 27, 3], dims=[256, 512, 1024, 2048], **kwargs)\n",
        "    if pretrained:\n",
        "        assert in_22k, \"only ImageNet-22K pre-trained ConvNeXt-XL is available; please set in_22k=True\"\n",
        "        url = model_urls['convnext_xlarge_22k']\n",
        "        checkpoint = torch.hub.load_state_dict_from_url(url=url, map_location=\"cpu\")\n",
        "        model.load_state_dict(checkpoint[\"model\"])\n",
        "    return model\n",
        "import sys \n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import os\n",
        "import glob\n",
        "import time\n",
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class CFG:\n",
        "    img_size = 256\n",
        "    n_frames = 10\n",
        "    \n",
        "    cnn_features = 256\n",
        "    lstm_hidden = 32\n",
        "    \n",
        "    n_fold = 5\n",
        "    n_epochs = 15\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        #self.map = nn.Conv2d(in_channels=3, out_channels=3, kernel_size=1)\n",
        "        self.net =convnext_base(pretrained=False)\n",
        "        #checkpoint = torch.load(\"../input/efficientnet-pytorch/efficientnet-b0-08094119.pth\")\n",
        "        #self.net.load_state_dict(checkpoint)\n",
        "        #out = out.view(-1, self.in_planes)\n",
        "        self.net.head = nn.Linear(in_features=1024, out_features=2, bias=True)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        #x = F.relu(self.map(x))\n",
        "        out,out1 = self.net(x)\n",
        "        return out,out1\n",
        "    \n",
        "#model=CNN()\n",
        "\n",
        "\n",
        "import timm\n",
        "print(\"Available Vision Transformer Models: \")\n",
        "print(timm.list_models(\"vit*\"))\n",
        "import timm\n",
        "#################################################### transformer model #############################\n",
        "class ViTBase16(nn.Module):\n",
        "    def __init__(self, n_classes, pretrained=False):\n",
        "\n",
        "        super(ViTBase16, self).__init__()\n",
        "\n",
        "        self.model = timm.create_model(\"vit_base_patch16_224_miil_in21k\", pretrained=pretrained)\n",
        "        #if pretrained:\n",
        "        #   self.model = timm.create_model(CFG['model_arch'], pretrained=True)\n",
        "            #self.model.load_state_dict(torch.load(MODEL_PATH))\n",
        "        self.model.head = nn.Linear(self.model.head.in_features, n_classes)\n",
        "        # self.model.head = nn.Sequential(nn.Linear(self.model.head.in_features, 512),\n",
        "        #                                 nn.Dropout(0.5),\n",
        "        #                                 nn.ReLU(True),\n",
        "        #                                 nn.Linear(512,n_classes),\n",
        "        #                                 )\n",
        "        #self.model.classifier = nn.Linear(self.model.classifier.in_features, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "    \n",
        "#model = ViTBase16(n_classes=2, pretrained=True).to(device)\n",
        "\n",
        "#model=nn.DataParallel(model)\n",
        "#model=model.to(device)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "################### variational autoencoder model\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "#from example_algos.models.nets import BasicEncoder, BasicGenerator\n",
        "\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NoOp(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"NoOp Pytorch Module.\n",
        "        Forwards the given input as is.\n",
        "        \"\"\"\n",
        "        super(NoOp, self).__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv Pytorch Conv Module\n",
        "        Has can have a Conv Op, a Normlization Op and a Non Linearity:\n",
        "        x = conv(x)\n",
        "        x = some_norm(x)\n",
        "        x = nonlin(x)\n",
        "\n",
        "        Args:\n",
        "            in_channels ([int]): [Number on input channels/ feature maps]\n",
        "            out_channels ([int]): [Number of ouput channels/ feature maps]\n",
        "            conv_op ([torch.nn.Module], optional): [Conv operation]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...)]. Defaults to None.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...)]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "        self.activation_params = activation_params\n",
        "        if self.activation_params is None:\n",
        "            self.activation_params = {}\n",
        "        self.normalization_params = normalization_params\n",
        "        if self.normalization_params is None:\n",
        "            self.normalization_params = {}\n",
        "\n",
        "        self.conv = None\n",
        "        if conv_op is not None and not isinstance(conv_op, str):\n",
        "            self.conv = conv_op(in_channels, out_channels, **self.conv_params)\n",
        "\n",
        "        self.normalization = None\n",
        "        if normalization_op is not None and not isinstance(normalization_op, str):\n",
        "            self.normalization = normalization_op(out_channels, **self.normalization_params)\n",
        "\n",
        "        self.activation = None\n",
        "        if activation_op is not None and not isinstance(activation_op, str):\n",
        "            self.activation = activation_op(**self.activation_params)\n",
        "\n",
        "    def forward(self, input, conv_add_input=None, normalization_add_input=None, activation_add_input=None):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        if self.conv is not None:\n",
        "            if conv_add_input is None:\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.conv(x, **conv_add_input)\n",
        "\n",
        "        if self.normalization is not None:\n",
        "            if normalization_add_input is None:\n",
        "                x = self.normalization(x)\n",
        "            else:\n",
        "                x = self.normalization(x, **normalization_add_input)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            if activation_add_input is None:\n",
        "                x = self.activation(x)\n",
        "            else:\n",
        "                x = self.activation(x, **activation_add_input)\n",
        "\n",
        "        # nn.functional.dropout(x, p=0.95, training=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs: int,\n",
        "        n_featmaps: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size)\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions]\n",
        "            n_featmaps ([type]): [Feature map size of the conv]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.n_convs):\n",
        "            conv_layer = ConvModule(\n",
        "                n_featmaps,\n",
        "                n_featmaps,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=conv_params,\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "            self.conv_list.append(conv_layer)\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        for conv_layer in self.conv_list:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs,\n",
        "        n_featmaps,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size) and a skip/ residual connection:\n",
        "        x = input\n",
        "        x = conv_block(x)\n",
        "        out = x + input\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions in the conv block]\n",
        "            n_featmaps ([type]): [Feature map size of the conv block]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_block = ConvBlock(\n",
        "            n_featmaps,\n",
        "            n_convs,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        out = x + input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Basic Generator\n",
        "class BasicGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(256, 128, 64),\n",
        "        upsample_op=nn.ConvTranspose2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Generator/ Decoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used.\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (256, 128, 64).\n",
        "            upsample_op ([torch.nn.Module], optional): [Upsampling operation used, to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If Latent dimesion is a z_dim x 1 x 1 vector (True) or if allows spatial resolution not to be 1x1 (z_dim x H x W) (False) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        super(BasicGenerator, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_ = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        elif len(fmap_sizes) < 2:\n",
        "            raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        # We need to know how many layers we will use at the beginning\n",
        "        input_size_new = input_size_ // (2 ** len(fmap_sizes))\n",
        "        if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "            raise AttributeError(\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### Start block\n",
        "        start_block = []\n",
        "\n",
        "        if not to_1x1:\n",
        "            kernel_size_start = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_start = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.start = ConvModule(\n",
        "                z_dim,\n",
        "                h_size_bot,\n",
        "                conv_op=upsample_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_start, stride=1, padding=0, bias=False),\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "\n",
        "            input_size_new = input_size_new * 2\n",
        "        else:\n",
        "            self.start = NoOp()\n",
        "\n",
        "        ### Middle block (Done until we reach ? x input_size/2 x input_size/2)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=upsample_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new * 2\n",
        "\n",
        "        ### End block\n",
        "        self.end = ConvModule(\n",
        "            h_size_bot,\n",
        "            n_channels,\n",
        "            conv_op=upsample_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=None,\n",
        "            activation_op=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Basic Encoder\n",
        "class BasicEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(64, 128, 256),\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Encoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used. \n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (64, 128, 256).\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(BasicEncoder, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_new = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        # elif len(fmap_sizes) < 2:\n",
        "        #     raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        ### Start block\n",
        "        self.start = ConvModule(\n",
        "            n_channels,\n",
        "            h_size_bot,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params={},\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "        input_size_new = input_size_new // 2\n",
        "\n",
        "        ### Middle block (Done until we reach ? x 4 x 4)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=conv_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new // 2\n",
        "\n",
        "            if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "                raise (\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### End block\n",
        "        if not to_1x1:\n",
        "            kernel_size_end = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_end = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.end = ConvModule(\n",
        "                h_size_bot,\n",
        "                z_dim,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_end, stride=1, padding=0, bias=False),\n",
        "                normalization_op=None,\n",
        "                activation_op=None,\n",
        "            )\n",
        "\n",
        "            if to_1x1:\n",
        "                self.output_size = (z_dim, 1, 1)\n",
        "            else:\n",
        "                self.output_size = (z_dim, *[i - (j - 1) for i, j in zip(input_size_new, kernel_size_end)])\n",
        "        else:\n",
        "            self.end = NoOp()\n",
        "            self.output_size = input_size_new\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic VAE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim * 2,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, sample=True, no_dist=False, **kwargs):\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        mu, log_std = torch.chunk(y1, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        z_dist = dist.Normal(mu, std)\n",
        "        if sample:\n",
        "            z_sample = z_dist.rsample()\n",
        "        else:\n",
        "            z_sample = mu\n",
        "\n",
        "        x_rec = self.dec(z_sample)\n",
        "\n",
        "        if no_dist:\n",
        "            return x_rec\n",
        "        else:\n",
        "            return x_rec, z_dist\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a sample and returns the paramters for the approx inference dist. (Normal)\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): The input to encode\n",
        "\n",
        "        Returns:\n",
        "            mu : The mean used to parameterized a Normal distribution\n",
        "            std: The standard deviation used to parameterized a Normal distribution\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        mu, log_std = torch.chunk(enc, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample, used the generative model (decode = mu_{gen}(z) as used in p(x|z) = N(x | mu_{gen}(z), 1) ).\n",
        "\n",
        "        Args:\n",
        "            inpt ([type]): A sample from the latent space to decode\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        x_rec = self.dec(inpt, **kwargs)\n",
        "        return x_rec\n",
        "\n",
        "\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=1024,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic AE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        x_rec = self.dec(y1)\n",
        "\n",
        "        return x_rec\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a input sample to a latent space sample\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): Input sample\n",
        "\n",
        "        Returns:\n",
        "            enc: Encoded input sample in the latent space\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample back to the input space\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): [Latent space sample]\n",
        "\n",
        "        Returns:\n",
        "            [rec]: [Encoded latent sample back in the input space]\n",
        "        \"\"\"\n",
        "        rec = self.dec(inpt, **kwargs)\n",
        "        return rec\n",
        "\n",
        "z_dim=512\n",
        "model_feature_map_sizes=(16, 64, 256, 1024)\n",
        "    \n",
        "import torch\n",
        "input_shape=((1, 1, 128, 128))\n",
        "#input_shape(0).shape\n",
        "#c,h,y=input_shape.size()[0],input_shape.size()[1],input_shape.size()[2]\n",
        "\n",
        "#model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.distributions as dist\n",
        "\n",
        "#from example_algos.models.nets import BasicEncoder, BasicGenerator\n",
        "\n",
        "import warnings\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class NoOp(nn.Module):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        \"\"\"NoOp Pytorch Module.\n",
        "        Forwards the given input as is.\n",
        "        \"\"\"\n",
        "        super(NoOp, self).__init__()\n",
        "\n",
        "    def forward(self, x, *args, **kwargs):\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvModule(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels: int,\n",
        "        out_channels: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv Pytorch Conv Module\n",
        "        Has can have a Conv Op, a Normlization Op and a Non Linearity:\n",
        "        x = conv(x)\n",
        "        x = some_norm(x)\n",
        "        x = nonlin(x)\n",
        "\n",
        "        Args:\n",
        "            in_channels ([int]): [Number on input channels/ feature maps]\n",
        "            out_channels ([int]): [Number of ouput channels/ feature maps]\n",
        "            conv_op ([torch.nn.Module], optional): [Conv operation]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...)]. Defaults to None.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...)]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvModule, self).__init__()\n",
        "\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "        self.activation_params = activation_params\n",
        "        if self.activation_params is None:\n",
        "            self.activation_params = {}\n",
        "        self.normalization_params = normalization_params\n",
        "        if self.normalization_params is None:\n",
        "            self.normalization_params = {}\n",
        "\n",
        "        self.conv = None\n",
        "        if conv_op is not None and not isinstance(conv_op, str):\n",
        "            self.conv = conv_op(in_channels, out_channels, **self.conv_params)\n",
        "\n",
        "        self.normalization = None\n",
        "        if normalization_op is not None and not isinstance(normalization_op, str):\n",
        "            self.normalization = normalization_op(out_channels, **self.normalization_params)\n",
        "\n",
        "        self.activation = None\n",
        "        if activation_op is not None and not isinstance(activation_op, str):\n",
        "            self.activation = activation_op(**self.activation_params)\n",
        "\n",
        "    def forward(self, input, conv_add_input=None, normalization_add_input=None, activation_add_input=None):\n",
        "\n",
        "        x = input\n",
        "\n",
        "        if self.conv is not None:\n",
        "            if conv_add_input is None:\n",
        "                x = self.conv(x)\n",
        "            else:\n",
        "                x = self.conv(x, **conv_add_input)\n",
        "\n",
        "        if self.normalization is not None:\n",
        "            if normalization_add_input is None:\n",
        "                x = self.normalization(x)\n",
        "            else:\n",
        "                x = self.normalization(x, **normalization_add_input)\n",
        "\n",
        "        if self.activation is not None:\n",
        "            if activation_add_input is None:\n",
        "                x = self.activation(x)\n",
        "            else:\n",
        "                x = self.activation(x, **activation_add_input)\n",
        "\n",
        "        # nn.functional.dropout(x, p=0.95, training=True)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ConvBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs: int,\n",
        "        n_featmaps: int,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size)\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions]\n",
        "            n_featmaps ([type]): [Feature map size of the conv]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(ConvBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_list = nn.ModuleList()\n",
        "\n",
        "        for i in range(self.n_convs):\n",
        "            conv_layer = ConvModule(\n",
        "                n_featmaps,\n",
        "                n_featmaps,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=conv_params,\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "            self.conv_list.append(conv_layer)\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        for conv_layer in self.conv_list:\n",
        "            x = conv_layer(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_convs,\n",
        "        n_featmaps,\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=nn.BatchNorm2d,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "    ):\n",
        "        \"\"\"Basic Conv block with repeated conv, build up from repeated @ConvModules (with same/fixed feature map size) and a skip/ residual connection:\n",
        "        x = input\n",
        "        x = conv_block(x)\n",
        "        out = x + input\n",
        "\n",
        "        Args:\n",
        "            n_convs ([type]): [Number of convolutions in the conv block]\n",
        "            n_featmaps ([type]): [Feature map size of the conv block]\n",
        "            conv_op ([torch.nn.Module], optional): [Convulioton operation -> see ConvModule ]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to None.\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(ResBlock, self).__init__()\n",
        "\n",
        "        self.n_featmaps = n_featmaps\n",
        "        self.n_convs = n_convs\n",
        "        self.conv_params = conv_params\n",
        "        if self.conv_params is None:\n",
        "            self.conv_params = {}\n",
        "\n",
        "        self.conv_block = ConvBlock(\n",
        "            n_featmaps,\n",
        "            n_convs,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "\n",
        "    def forward(self, input, **frwd_params):\n",
        "        x = input\n",
        "        x = self.conv_block(x)\n",
        "\n",
        "        out = x + input\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# Basic Generator\n",
        "class BasicGenerator(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(256, 128, 64),\n",
        "        upsample_op=nn.ConvTranspose2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Generator/ Decoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used.\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (256, 128, 64).\n",
        "            upsample_op ([torch.nn.Module], optional): [Upsampling operation used, to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If Latent dimesion is a z_dim x 1 x 1 vector (True) or if allows spatial resolution not to be 1x1 (z_dim x H x W) (False) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "\n",
        "        super(BasicGenerator, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=4, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_ = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        elif len(fmap_sizes) < 2:\n",
        "            raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        # We need to know how many layers we will use at the beginning\n",
        "        input_size_new = input_size_ // (2 ** len(fmap_sizes))\n",
        "        if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "            raise AttributeError(\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### Start block\n",
        "        start_block = []\n",
        "\n",
        "        if not to_1x1:\n",
        "            kernel_size_start = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_start = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.start = ConvModule(\n",
        "                z_dim,\n",
        "                h_size_bot,\n",
        "                conv_op=upsample_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_start, stride=1, padding=0, bias=False),\n",
        "                normalization_op=normalization_op,\n",
        "                normalization_params=normalization_params,\n",
        "                activation_op=activation_op,\n",
        "                activation_params=activation_params,\n",
        "            )\n",
        "\n",
        "            input_size_new = input_size_new * 2\n",
        "        else:\n",
        "            self.start = NoOp()\n",
        "\n",
        "        ### Middle block (Done until we reach ? x input_size/2 x input_size/2)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=upsample_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new * 2\n",
        "\n",
        "        ### End block\n",
        "        self.end = ConvModule(\n",
        "            h_size_bot,\n",
        "            n_channels,\n",
        "            conv_op=upsample_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=None,\n",
        "            activation_op=None,\n",
        "        )\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "# Basic Encoder\n",
        "class BasicEncoder(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(64, 128, 256),\n",
        "        conv_op=nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        normalization_op=NoOp,\n",
        "        normalization_params=None,\n",
        "        activation_op=nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=NoOp,\n",
        "        block_params=None,\n",
        "        to_1x1=True,\n",
        "    ):\n",
        "        \"\"\"Basic configureable Encoder.\n",
        "        Allows for mutilple \"feature-map\" levels defined by the feature map size, where for each feature map size a conv operation + optional conv block is used. \n",
        "\n",
        "        Args:\n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim).\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (64, 128, 256).\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ]. Defaults to True.\n",
        "        \"\"\"\n",
        "        super(BasicEncoder, self).__init__()\n",
        "\n",
        "        if conv_params is None:\n",
        "            conv_params = dict(kernel_size=3, stride=2, padding=1, bias=False)\n",
        "        if block_op is None:\n",
        "            block_op = NoOp\n",
        "        if block_params is None:\n",
        "            block_params = {}\n",
        "\n",
        "        n_channels = input_size[0]\n",
        "        input_size_new = np.array(input_size[1:])\n",
        "\n",
        "        if not isinstance(fmap_sizes, list) and not isinstance(fmap_sizes, tuple):\n",
        "            raise AttributeError(\"fmap_sizes has to be either a list or tuple or an int\")\n",
        "        # elif len(fmap_sizes) < 2:\n",
        "        #     raise AttributeError(\"fmap_sizes has to contain at least three elements\")\n",
        "        else:\n",
        "            h_size_bot = fmap_sizes[0]\n",
        "\n",
        "        ### Start block\n",
        "        self.start = ConvModule(\n",
        "            n_channels,\n",
        "            h_size_bot,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params={},\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "        )\n",
        "        input_size_new = input_size_new // 2\n",
        "\n",
        "        ### Middle block (Done until we reach ? x 4 x 4)\n",
        "        self.middle_blocks = nn.ModuleList()\n",
        "\n",
        "        for h_size_top in fmap_sizes[1:]:\n",
        "\n",
        "            self.middle_blocks.append(block_op(h_size_bot, **block_params))\n",
        "\n",
        "            self.middle_blocks.append(\n",
        "                ConvModule(\n",
        "                    h_size_bot,\n",
        "                    h_size_top,\n",
        "                    conv_op=conv_op,\n",
        "                    conv_params=conv_params,\n",
        "                    normalization_op=normalization_op,\n",
        "                    normalization_params={},\n",
        "                    activation_op=activation_op,\n",
        "                    activation_params=activation_params,\n",
        "                )\n",
        "            )\n",
        "\n",
        "            h_size_bot = h_size_top\n",
        "            input_size_new = input_size_new // 2\n",
        "\n",
        "            if np.min(input_size_new) < 2 and z_dim is not None:\n",
        "                raise (\"fmap_sizes to long, one image dimension has already perished\")\n",
        "\n",
        "        ### End block\n",
        "        if not to_1x1:\n",
        "            kernel_size_end = [min(conv_params[\"kernel_size\"], i) for i in input_size_new]\n",
        "        else:\n",
        "            kernel_size_end = input_size_new.tolist()\n",
        "\n",
        "        if z_dim is not None:\n",
        "            self.end = ConvModule(\n",
        "                h_size_bot,\n",
        "                z_dim,\n",
        "                conv_op=conv_op,\n",
        "                conv_params=dict(kernel_size=kernel_size_end, stride=1, padding=0, bias=False),\n",
        "                normalization_op=None,\n",
        "                activation_op=None,\n",
        "            )\n",
        "\n",
        "            if to_1x1:\n",
        "                self.output_size = (z_dim, 1, 1)\n",
        "            else:\n",
        "                self.output_size = (z_dim, *[i - (j - 1) for i, j in zip(input_size_new, kernel_size_end)])\n",
        "        else:\n",
        "            self.end = NoOp()\n",
        "            self.output_size = input_size_new\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "        output = self.start(inpt, **kwargs)\n",
        "        for middle in self.middle_blocks:\n",
        "            output = middle(output, **kwargs)\n",
        "        output = self.end(output, **kwargs)\n",
        "        return output\n",
        "\n",
        "\n",
        "class VAE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=256,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic VAE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "\n",
        "        super(VAE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim * 2,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, sample=True, no_dist=False, **kwargs):\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        mu, log_std = torch.chunk(y1, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        z_dist = dist.Normal(mu, std)\n",
        "        if sample:\n",
        "            z_sample = z_dist.rsample()\n",
        "        else:\n",
        "            z_sample = mu\n",
        "\n",
        "        x_rec = self.dec(z_sample)\n",
        "\n",
        "        if no_dist:\n",
        "            return x_rec\n",
        "        else:\n",
        "            return x_rec, z_dist\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a sample and returns the paramters for the approx inference dist. (Normal)\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): The input to encode\n",
        "\n",
        "        Returns:\n",
        "            mu : The mean used to parameterized a Normal distribution\n",
        "            std: The standard deviation used to parameterized a Normal distribution\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        mu, log_std = torch.chunk(enc, 2, dim=1)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample, used the generative model (decode = mu_{gen}(z) as used in p(x|z) = N(x | mu_{gen}(z), 1) ).\n",
        "\n",
        "        Args:\n",
        "            inpt ([type]): A sample from the latent space to decode\n",
        "\n",
        "        Returns:\n",
        "            [type]: [description]\n",
        "        \"\"\"\n",
        "        x_rec = self.dec(inpt, **kwargs)\n",
        "        return x_rec\n",
        "\n",
        "\n",
        "class AE(torch.nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size,\n",
        "        z_dim=1024,\n",
        "        fmap_sizes=(16, 64, 256, 1024),\n",
        "        to_1x1=True,\n",
        "        conv_op=torch.nn.Conv2d,\n",
        "        conv_params=None,\n",
        "        tconv_op=torch.nn.ConvTranspose2d,\n",
        "        tconv_params=None,\n",
        "        normalization_op=None,\n",
        "        normalization_params=None,\n",
        "        activation_op=torch.nn.LeakyReLU,\n",
        "        activation_params=None,\n",
        "        block_op=None,\n",
        "        block_params=None,\n",
        "        *args,\n",
        "        **kwargs\n",
        "    ):\n",
        "        \"\"\"Basic AE build up of a symetric BasicEncoder (Encoder) and BasicGenerator (Decoder)\n",
        "\n",
        "        Args:\n",
        "            input_size ((int, int, int): Size of the input in format CxHxW): \n",
        "            z_dim (int, optional): [description]. Dimension of the latent / Input dimension (C channel-dim). Defaults to 256\n",
        "            fmap_sizes (tuple, optional): [Defines the Upsampling-Levels of the generator, list/ tuple of ints, where each \n",
        "                                            int defines the number of feature maps in the layer]. Defaults to (16, 64, 256, 1024).\n",
        "            to_1x1 (bool, optional): [If True, then the last conv layer goes to a latent dimesion is a z_dim x 1 x 1 vector (similar to fully connected) \n",
        "                                        or if False allows spatial resolution not to be 1x1 (z_dim x H x W, uses the in the conv_params given conv-kernel-size) ].\n",
        "                                        Defaults to True.\n",
        "            conv_op ([torch.nn.Module], optional): [Convolutioon operation used in the encoder to downsample to a new level/ featuremap size]. Defaults to nn.Conv2d.\n",
        "            conv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            tconv_op ([torch.nn.Module], optional): [Upsampling/ Transposed Conv operation used in the decoder to upsample to a new level/ featuremap size]. Defaults to nn.ConvTranspose2d.\n",
        "            tconv_params ([dict], optional): [Init parameters for the conv operation]. Defaults to dict(kernel_size=3, stride=2, padding=1, bias=False).\n",
        "            normalization_op ([torch.nn.Module], optional): [Normalization Operation (e.g. BatchNorm, InstanceNorm,...) -> see ConvModule]. Defaults to nn.BatchNorm2d.\n",
        "            normalization_params ([dict], optional): [Init parameters for the normalization operation]. Defaults to None.\n",
        "            activation_op ([torch.nn.Module], optional): [Actiovation Operation/ Non-linearity (e.g. ReLU, Sigmoid,...) -> see ConvModule]. Defaults to nn.LeakyReLU.\n",
        "            activation_params ([dict], optional): [Init parameters for the activation operation]. Defaults to None.\n",
        "            block_op ([torch.nn.Module], optional): [Block operation used for each feature map size after each upsample op of e.g. ConvBlock/ ResidualBlock]. Defaults to NoOp.\n",
        "            block_params ([dict], optional): [Init parameters for the block operation]. Defaults to None.\n",
        "        \"\"\"\n",
        "        super(AE, self).__init__()\n",
        "\n",
        "        input_size_enc = list(input_size)\n",
        "        input_size_dec = list(input_size)\n",
        "\n",
        "        self.enc = BasicEncoder(\n",
        "            input_size=input_size_enc,\n",
        "            fmap_sizes=fmap_sizes,\n",
        "            z_dim=z_dim,\n",
        "            conv_op=conv_op,\n",
        "            conv_params=conv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "        self.dec = BasicGenerator(\n",
        "            input_size=input_size_dec,\n",
        "            fmap_sizes=fmap_sizes[::-1],\n",
        "            z_dim=z_dim,\n",
        "            upsample_op=tconv_op,\n",
        "            conv_params=tconv_params,\n",
        "            normalization_op=normalization_op,\n",
        "            normalization_params=normalization_params,\n",
        "            activation_op=activation_op,\n",
        "            activation_params=activation_params,\n",
        "            block_op=block_op,\n",
        "            block_params=block_params,\n",
        "            to_1x1=to_1x1,\n",
        "        )\n",
        "\n",
        "        self.hidden_size = self.enc.output_size\n",
        "\n",
        "    def forward(self, inpt, **kwargs):\n",
        "\n",
        "        y1 = self.enc(inpt, **kwargs)\n",
        "\n",
        "        x_rec = self.dec(y1)\n",
        "\n",
        "        return x_rec\n",
        "\n",
        "    def encode(self, inpt, **kwargs):\n",
        "        \"\"\"Encodes a input sample to a latent space sample\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): Input sample\n",
        "\n",
        "        Returns:\n",
        "            enc: Encoded input sample in the latent space\n",
        "        \"\"\"\n",
        "        enc = self.enc(inpt, **kwargs)\n",
        "        return enc\n",
        "\n",
        "    def decode(self, inpt, **kwargs):\n",
        "        \"\"\"Decodes a latent space sample back to the input space\n",
        "\n",
        "        Args:\n",
        "            inpt ([tensor]): [Latent space sample]\n",
        "\n",
        "        Returns:\n",
        "            [rec]: [Encoded latent sample back in the input space]\n",
        "        \"\"\"\n",
        "        rec = self.dec(inpt, **kwargs)\n",
        "        return rec\n",
        "\n",
        "z_dim=512\n",
        "model_feature_map_sizes=(16, 64, 256, 1024)\n",
        "    \n",
        "import torch\n",
        "input_shape=((1, 1, 128, 128))\n",
        "#input_shape(0).shape\n",
        "#c,h,y=input_shape.size()[0],input_shape.size()[1],input_shape.size()[2]\n",
        "\n",
        "model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)\n",
        "#model=nn.DataParallel(model)\n",
        "\n",
        "patha='/content/drive/MyDrive/model_weights1.pth'\n",
        "#trained=torch.load(patha)\n",
        "#model1.load_state_dict(trained)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#model=nn.DataParallel(model)\n",
        "#model=model.to(device)\n",
        "from evalutils import ClassificationAlgorithm\n",
        "from evalutils.validators import (\n",
        "    UniquePathIndicesValidator,\n",
        "    UniqueImagesValidator,\n",
        ")\n",
        "from evalutils.io import ImageLoader\n",
        "\n",
        "import ttach as tta\n",
        "class DummyLoader(ImageLoader):\n",
        "    @staticmethod\n",
        "    def load_image(fname):\n",
        "        return str(fname)\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def hash_image(image):\n",
        "        return hash(image)\n",
        "\n",
        "\n",
        "class airogs_algorithm(ClassificationAlgorithm):\n",
        "    def __init__(self):\n",
        "        super().__init__(\n",
        "            validators=dict(\n",
        "                input_image=(\n",
        "                    UniqueImagesValidator(),\n",
        "                    UniquePathIndicesValidator(),\n",
        "                )\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        self._file_loaders = dict(input_image=DummyLoader())\n",
        "\n",
        "        self.output_keys = [\"multiple-referable-glaucoma-likelihoods\", \n",
        "                            \"multiple-referable-glaucoma-binary\",\n",
        "                            \"multiple-ungradability-scores\",\n",
        "                            \"multiple-ungradability-binary\"]\n",
        "    \n",
        "    def load(self):\n",
        "        for key, file_loader in self._file_loaders.items():\n",
        "            fltr = (\n",
        "                self._file_filters[key] if key in self._file_filters else None\n",
        "            )\n",
        "            self._cases[key] = self._load_cases(\n",
        "                folder=Path(\"/content/drive/MyDrive/Irgos_challenege2022/airogs-example-algorithm-master/test/images/color-fundus/\"),\n",
        "                file_loader=file_loader,\n",
        "                file_filter=fltr,\n",
        "            )\n",
        "\n",
        "        pass\n",
        "    \n",
        "    def combine_dicts(self, dicts):\n",
        "        out = {}\n",
        "        for d in dicts:\n",
        "            for k, v in d.items():\n",
        "                if k not in out:\n",
        "                    out[k] = []\n",
        "                out[k].append(v)\n",
        "        return out\n",
        "    \n",
        "    def process_case(self, *, idx, case):\n",
        "        # Load and test the image(s) for this case\n",
        "        if case.path.suffix == '.tiff':\n",
        "            results = []\n",
        "            #predc=[]\n",
        "            with tifffile.TiffFile(case.path) as stack:\n",
        "                for page in tqdm.tqdm(stack.pages):\n",
        "                    input_image_array = page.asarray()\n",
        "                    \n",
        "                    #print(input_image_array.shape)\n",
        "                    input_image_array= cv2.resize(input_image_array, dsize=(256, 256), interpolation=cv2.INTER_CUBIC)\n",
        "                    input_image_array=((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min()))\n",
        "                    #np_img=input_image_array\n",
        "                    #torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "                    #torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "                    #print(torch_array.shape)\n",
        "                    #plt.imshow(input_image_array)\n",
        "                    #print(input_image_array.min())\n",
        "                    #print(input_image_array.max())\n",
        "                    #p1,p2=self.predict(input_image_array=input_image_array)\n",
        "                    results.append(self.predict(input_image_array=input_image_array))\n",
        "                    #predc.append(p2)\n",
        "        else:\n",
        "            input_image = SimpleITK.ReadImage(str(case.path))\n",
        "            input_image_array = SimpleITK.GetArrayFromImage(input_image)\n",
        "            print(input_image_array.shape)\n",
        "            results = [self.predict(input_image_array=input_image_array)]\n",
        "        \n",
        "        results = self.combine_dicts(results)\n",
        "\n",
        "        # Test classification output\n",
        "        if not isinstance(results, dict):\n",
        "            raise ValueError(\"Expected a dictionary as output\")\n",
        "\n",
        "        return results\n",
        "    \n",
        "    #pc11=[]\n",
        "\n",
        "    def predict(self, *, input_image_array: np.ndarray) -> Dict:\n",
        "        # From here, use the input_image to predict the output\n",
        "        # We are using a not-so-smart algorithm to predict the output, you'll want to do your model inference here\n",
        "\n",
        "        # Replace starting here\n",
        "        #print(input_image_array)\n",
        "        #np_img=input_image_array\n",
        "        #torch_array=torch.from_numpy(np_img).squeeze(axis=0)\n",
        "        #np_img=input_image_array\n",
        "        #torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "        #torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "        #torch_array=torch_array.float().to(device)\n",
        "        ####### load pytorch model for prediction\n",
        "        #model=CNN()\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        #pretrained=torch.load(path)\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        #model.load_state_dict(pretrained)\n",
        "        #y_pred,_=model(torch_array)\n",
        "        #pc1= torch.softmax(y_pred, dim=1)\n",
        "        #predict1=pc1.detach().cpu().numpy()\n",
        "        #print(predict1)\n",
        "        #pc11.append(predict1)\n",
        "        \n",
        "        #rg_likelihood = ((input_image_array - input_image_array.min()) / (input_image_array.max() - input_image_array.min())).mean()\n",
        "        #rg_binary = bool(rg_likelihood > .2)\n",
        "        #ungradability_score = rg_likelihood * 15\n",
        "        #ungradability_binary = bool(rg_likelihood < .2)\n",
        "        \n",
        "        np_img=input_image_array\n",
        "        torch_array=torch.from_numpy(np_img).unsqueeze(axis=0)\n",
        "        torch_array=torch_array.permute(0, 3, 1, 2)\n",
        "        torch_array=torch_array.float().to(device)\n",
        "        ####### load pytorch model for prediction\n",
        "        model=CNN()\n",
        "        import ttach as tta\n",
        "        \n",
        "        model=nn.DataParallel(model)\n",
        "        model=model.to(device)\n",
        "        #model = ViTBase16(n_classes=2, pretrained=False).to(device)\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        pretrained=torch.load(path)\n",
        "        #model=nn.DataParallel(model)\n",
        "        #model=model.to(device)\n",
        "        model.load_state_dict(pretrained)\n",
        "        model1 = AE(input_size=(3,128,128), z_dim=z_dim, fmap_sizes=model_feature_map_sizes)\n",
        "        #patha='/content/drive/MyDrive/Irgos_challenege2022/model_weights.pth'\n",
        "        model1=nn.DataParallel(model1)\n",
        "        trained=torch.load(patha)\n",
        "        model1.load_state_dict(trained)\n",
        "        with torch.no_grad():\n",
        "          #tta_model = tta.ClassificationTTAWrapper(model, tta.aliases.flip_transform())\n",
        "          y_pred,_=model(torch_array)\n",
        "          pc1= torch.softmax(y_pred, dim=1)\n",
        "          pc1=pc1.squeeze(axis=0)\n",
        "          predic0,predic1=pc1.detach().cpu().numpy()\n",
        "          #p2=predic1\n",
        "          #predict11.append(p2)\n",
        "          #predic00.append(predic0)\n",
        "          input_image_array1= cv2.resize(input_image_array, dsize=(128, 128), interpolation=cv2.INTER_CUBIC)\n",
        "          input_image_array1=((input_image_array1 - input_image_array1.min()) / (input_image_array1.max() - input_image_array1.min()))\n",
        "          #print(input_image_array1.shape)\n",
        "          np_img1=input_image_array1\n",
        "          torch_array1=torch.from_numpy(np_img1).unsqueeze(axis=0)\n",
        "          torch_array1=torch_array1.permute(0, 3, 1, 2)\n",
        "          torch_array1=torch_array1.float().to(device)\n",
        "          model1=model1.to(device)\n",
        "          torch_array1=torch_array1.to(device)\n",
        "          batch_rec = model1(torch_array1)\n",
        "          loss = torch.mean(torch.pow(torch_array1 - batch_rec, 2), dim=(1, 2, 3))\n",
        "          #slice_scores += loss.cpu().tolist()\n",
        "          #batch_rec1=batch_rec.squeeze(axis=0).detach().cpu().numpy()\n",
        "          ###### output probabilties computed here\n",
        "          rg_likelihood=float(predic1)\n",
        "          rg_binary=(rg_likelihood>0.5)\n",
        "          #rg_likelihood=str(rg_likelihood1)\n",
        "          #rg_binary=str(rg_binary1)\n",
        "          un_gradable=predic0\n",
        "          predb0=(rg_likelihood<0.5)\n",
        "          w=loss.cpu().numpy()\n",
        "          w=(w>0.2)\n",
        "          ungradability_score=np.array(w or predb0)\n",
        "          #ungradability_score=str(ungradability_score1)\n",
        "          ungradability_score=float(np.squeeze(ungradability_score))\n",
        "          ungradability_binary=bool(ungradability_score>0)\n",
        "          #ungradability_score=str(ungradability_binary1)\n",
        "\n",
        "          #print(rg_likelihood)\n",
        "          #print(rg_binary)\n",
        "          #print(ungradability_score)\n",
        "          #print(ungradability_binary)\n",
        "\n",
        "\n",
        "          #likelihood1.append(likelihood)\n",
        "          #rg_binary1.append(rg_binary)\n",
        "          #ungradability_score1.append(ungradability_score)\n",
        "          #ungradability_binary1.append(ungradability_binary)\n",
        "        \n",
        "        \n",
        "        # to here with your inference algorithm\n",
        "\n",
        "        out = {\n",
        "            \"multiple-referable-glaucoma-likelihoods\": rg_likelihood,\n",
        "            \"multiple-referable-glaucoma-binary\": rg_binary,\n",
        "            \"multiple-ungradability-scores\": ungradability_score,\n",
        "            \"multiple-ungradability-binary\": ungradability_binary\n",
        "        }\n",
        "\n",
        "        return out\n",
        "\n",
        "    def save(self):\n",
        "        for key in self.output_keys:\n",
        "            with open(f\"/content/drive/MyDrive/Irgos_challenege2022/output4/{key}.json\", \"w\") as f:\n",
        "                out = []\n",
        "                for case_result in self._case_results:\n",
        "                    out += case_result[key]\n",
        "                json.dump(out, f)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    airogs_algorithm().process()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84VQUNU9VcsH",
        "outputId": "a4956281-56b7-43e9-a316-4cb277f80ae9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Vision Transformer Models: \n",
            "['vit_base_patch8_224', 'vit_base_patch8_224_in21k', 'vit_base_patch16_224', 'vit_base_patch16_224_in21k', 'vit_base_patch16_224_miil', 'vit_base_patch16_224_miil_in21k', 'vit_base_patch16_384', 'vit_base_patch16_sam_224', 'vit_base_patch32_224', 'vit_base_patch32_224_in21k', 'vit_base_patch32_384', 'vit_base_patch32_sam_224', 'vit_base_r26_s32_224', 'vit_base_r50_s16_224', 'vit_base_r50_s16_224_in21k', 'vit_base_r50_s16_384', 'vit_base_resnet26d_224', 'vit_base_resnet50_224_in21k', 'vit_base_resnet50_384', 'vit_base_resnet50d_224', 'vit_giant_patch14_224', 'vit_gigantic_patch14_224', 'vit_huge_patch14_224', 'vit_huge_patch14_224_in21k', 'vit_large_patch16_224', 'vit_large_patch16_224_in21k', 'vit_large_patch16_384', 'vit_large_patch32_224', 'vit_large_patch32_224_in21k', 'vit_large_patch32_384', 'vit_large_r50_s32_224', 'vit_large_r50_s32_224_in21k', 'vit_large_r50_s32_384', 'vit_small_patch16_224', 'vit_small_patch16_224_in21k', 'vit_small_patch16_384', 'vit_small_patch32_224', 'vit_small_patch32_224_in21k', 'vit_small_patch32_384', 'vit_small_r26_s32_224', 'vit_small_r26_s32_224_in21k', 'vit_small_r26_s32_384', 'vit_small_resnet26d_224', 'vit_small_resnet50d_s16_224', 'vit_tiny_patch16_224', 'vit_tiny_patch16_224_in21k', 'vit_tiny_patch16_384', 'vit_tiny_r_s16_p8_224', 'vit_tiny_r_s16_p8_224_in21k', 'vit_tiny_r_s16_p8_384']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:39<00:00,  3.97s/it]\n"
          ]
        }
      ]
    }
  ]
}